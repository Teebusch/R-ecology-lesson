{
  "articles": [
    {
      "path": "00-before-we-start.html",
      "title": "Before we start",
      "author": [],
      "contents": "\n\nContents\nWhat is R? What is RStudio?\nWhy learn R?Not only is R free, but it is also open-source and cross-platform\n\nKnowing your way around RStudio\nGetting set upOrganizing your working directory\nThe working directory\n\nInteracting with R\nSeeking helpSearching function documentation with ? and ??\nAutomatic code completion\nPackage vignettes and cheat sheets\nFinding more functions and packages\nDealing with error messages\nAsking for help\n\nHow to learn more after the workshop?\nMore resources\n\n\n\n\n\nLearning Objectives\nBe able to explain what R and RStudio are, what they are used for, and how they relate to each other.\nDescribe the purpose of the RStudio Script, Console, Environment, and Plots panes.\nOrganize files and directories for a set of analyses as an R Project, and understand the purpose of the working directory.\nUse the built-in RStudio help interface to search for more information on R functions.\nDemonstrate how to provide sufficient information for troubleshooting with the R user community.\n\nWhat is R? What is RStudio?\nThe term “R” is used to refer to both the programming language and the software that interprets the scripts written using it.\nRStudio is currently a very popular way to not only write your R scripts but also to interact with the R software. To function correctly, RStudio needs R and therefore both need to be installed on your computer.\nWhy learn R?\nR does not involve lots of pointing and clicking, and that’s a good thing\nThe learning curve might be steeper than with other software, but with R, the results of your analysis do not rely on remembering a succession of pointing and clicking, but instead on a series of written commands, and that’s a good thing! So, if you want to redo your analysis because you collected more data, you don’t have to remember which button you clicked in which order to obtain your results; you just have to run your script again.\nWorking with scripts makes the steps you used in your analysis clear, and the code you write can be inspected by someone else who can give you feedback and spot mistakes.\nWorking with scripts forces you to have a deeper understanding of what you are doing, and facilitates your learning and comprehension of the methods you use.\nR code is great for reproducibility\nReproducibility is when someone else (including your future self) can obtain the same results from the same dataset when using the same analysis.\nR integrates with other tools to generate manuscripts from your code. If you collect more data, or fix a mistake in your dataset, the figures and the statistical tests in your manuscript are updated automatically.\nAn increasing number of journals and funding agencies expect analyses to be reproducible, so knowing R will give you an edge with these requirements.\nR is interdisciplinary and extensible\nWith 10,000+ packages that can be installed to extend its capabilities, R provides a framework that allows you to combine statistical approaches from many scientific disciplines to best suit the analytical framework you need to analyze your data. For instance, R has packages for image analysis, GIS, time series, population genetics, and a lot more.\nR works on data of all shapes and sizes\nThe skills you learn with R scale easily with the size of your dataset. Whether your dataset has hundreds or millions of lines, it won’t make much difference to you.\nR is designed for data analysis. It comes with special data structures and data types that make handling of missing data and statistical factors convenient.\nR can connect to spreadsheets, databases, and many other data formats, on your computer or on the web.\nR produces high-quality graphics\nThe plotting functionalities in R are endless, and allow you to adjust any aspect of your graph to convey most effectively the message from your data.\nR has a large and welcoming community\nThousands of people use R daily. Many of them are willing to help you through mailing lists and websites such as Stack Overflow, or on the RStudio community.\nNot only is R free, but it is also open-source and cross-platform\nAnyone can inspect the source code to see how R works. Because of this transparency, there is less chance for mistakes, and if you (or someone else) find some, you can report and fix bugs.\nKnowing your way around RStudio\nLet’s start by learning about RStudio, which is an Integrated Development Environment (IDE) for working with R.\nThe RStudio IDE open-source product is free under the Affero General Public License (AGPL) v3. The RStudio IDE is also available with a commercial license and priority email support from RStudio, PBC.\nWe will use RStudio IDE to write code, navigate the files on our computer, inspect the variables we are going to create, and visualize the plots we will generate. RStudio can also be used for other things (e.g., version control, developing packages, writing Shiny apps) that we will not cover during the workshop.\nRStudio interface screenshot. Clockwise from top left: Source, Environment/History, Files/Plots/Packages/Help/Viewer, Console.RStudio is divided into 4 “Panes”: the Source for your scripts and documents (top-left, in the default layout), your Environment/History (top-right) which shows all the objects in your working space (Environment) and your command history (History), your Files/Plots/Packages/Help/Viewer (bottom-right), and the R Console (bottom-left). The placement of these panes and their content can be customized (see menu, Tools -> Global Options -> Pane Layout).\nOne of the advantages of using RStudio is that all the information you need to write code is available in a single window. Additionally, with many shortcuts, autocompletion, and highlighting for the major file types you use while developing in R, RStudio will make typing easier and less error-prone.\nGetting set up\nIt is good practice to keep a set of related data, analyses, and text self-contained in a single folder, called the working directory. All of the scripts within this folder can then use relative paths to files that indicate where inside the project a file is located (as opposed to absolute paths, which point to where a file is on a specific computer). Working this way makes it a lot easier to move your project around on your computer and share it with others without worrying about whether or not the underlying scripts will still work.\nRStudio provides a helpful set of tools to do this through its “Projects” interface, which not only creates a working directory for you, but also remembers its location (allowing you to quickly navigate to it) and optionally preserves custom settings and open files to make it easier to resume work after a break. Go through the steps for creating an “R Project” for this tutorial below.\nStart RStudio.\nUnder the File menu, click on New Project. Choose New Directory, then New Project.\nEnter a name for this new folder (or “directory”), and choose a convenient location for it. This will be your working directory for the rest of the day (e.g., ~/data-carpentry).\nClick on Create Project.\nDownload the code handout, place it in your working directory and rename it (e.g., data-carpentry-script.R).\n(Optional) Set Preferences to ‘Never’ save workspace in RStudio.\nA workspace is your current working environment in R which includes any user-defined object. By default, all of these objects will be saved, and automatically loaded, when you reopen your project. Saving a workspace to .RData can be cumbersome, especially if you are working with larger datasets, and it can lead to hard to debug errors by having objects in memory you forgot you had. To turn that off, go to Tools –> ‘Global Options’ and select the ‘Never’ option for ‘Save workspace to .RData’ on exit.’\n\n\n\nOrganizing your working directory\nUsing a consistent folder structure across your projects will help keep things organized, and will also make it easy to find/file things in the future. This can be especially helpful when you have multiple projects. In general, you may create directories (folders) for scripts, data, and documents.\ndata_raw/ & data/ Use these folders to store raw data and intermediate datasets you may create for the need of a particular analysis. For the sake of transparency and provenance, you should always keep a copy of your raw data accessible and do as much of your data cleanup and preprocessing programmatically (i.e., with scripts, rather than manually) as possible. Separating raw data from processed data is also a good idea. For example, you could have files data_raw/tree_survey.plot1.txt and ...plot2.txt kept separate from a data/tree.survey.csv file generated by the scripts/01.preprocess.tree_survey.R script.\ndocuments/ This would be a place to keep outlines, drafts, and other text.\nscripts/ This would be the location to keep your R scripts for different analyses or plotting, and potentially a separate folder for your functions (more on that later).\nAdditional (sub)directories depending on your project needs.\nFor this workshop, we will need a data_raw/ folder to store our raw data, and we will use data/ for when we learn how to export data as CSV files, and a fig/ folder for the figures that we will save.\nUnder the Files tab on the right of the screen, click on New Folder and create a folder named data_raw within your newly created working directory (e.g., ~/data-carpentry/). (Alternatively, type dir.create(\"data_raw\") at your R console.) Repeat these operations to create a data and a fig folder.\nWe are going to keep the script in the root of our working directory because we are only going to use one file and it will make things easier.\nYour working directory should now look like this:\n\n\n\nThe working directory\nThe working directory is an important concept to understand. It is the place from where R will be looking for and saving the files. When you write code for your project, it should refer to files in relation to the root of your working directory and only need files within this structure.\nUsing RStudio projects makes this easy and ensures that your working directory is set properly. If you need to check it, you can use getwd(). If for some reason your working directory is not what it should be, you can change it in the RStudio interface by navigating in the file browser where your working directory should be, and clicking on the blue gear icon “More”, and select “Set As Working Directory”. Alternatively you can use setwd(\"/path/to/working/directory\") to reset your working directory. However, your scripts should not include this line because it will fail on someone else’s computer.\nInteracting with R\nThe basis of programming is that we write down instructions for the computer to follow, and then we tell the computer to follow those instructions. We write, or code, instructions in R because it is a common language that both the computer and we can understand. We call the instructions commands and we tell the computer to follow the instructions by executing (also called running) those commands.\nThere are two main ways of interacting with R: by using the console or by using script files (plain text files that contain your code). The console pane (in RStudio, the bottom left panel) is the place where commands written in the R language can be typed and executed immediately by the computer. It is also where the results will be shown for commands that have been executed. You can type commands directly into the console and press Enter to execute those commands, but they will be forgotten when you close the session.\nBecause we want our code and workflow to be reproducible, it is better to type the commands we want in the script editor, and save the script. This way, there is a complete record of what we did, and anyone (including our future selves!) can easily replicate the results on their computer.\nRStudio allows you to execute commands directly from the script editor by using the Ctrl + Enter shortcut (on Macs, Cmd + Return will work, too). The command on the current line in the script (indicated by the cursor) or all of the commands in the currently selected text will be sent to the console and executed when you press Ctrl + Enter. You can find other keyboard shortcuts in this RStudio cheatsheet about the RStudio IDE.\nAt some point in your analysis you may want to check the content of a variable or the structure of an object, without necessarily keeping a record of it in your script. You can type these commands and execute them directly in the console. RStudio provides the Ctrl + 1 and Ctrl + 2 shortcuts allow you to jump between the script and the console panes.\nIf R is ready to accept commands, the R console shows a > prompt. If it receives a command (by typing, copy-pasting or sent from the script editor using Ctrl + Enter), R will try to execute it, and when ready, will show the results and come back with a new > prompt to wait for new commands.\nIf R is still waiting for you to enter more data because it isn’t complete yet, the console will show a + prompt. It means that you haven’t finished entering a complete command. This is because you have not ‘closed’ a parenthesis or quotation, i.e. you don’t have the same number of left-parentheses as right-parentheses, or the same number of opening and closing quotation marks. When this happens, and you thought you finished typing your command, click inside the console window and press Esc; this will cancel the incomplete command and return you to the > prompt.\nSeeking help\nSearching function documentation with ? and ??\nIf you need help with a specific function, let’s say mean(), you can type ?mean or press F1 while your cursor is on the function name. If you are looking for a function to do a particular task, but don’t know the function name, you can use the double question mark ??, for example ??kruskall. Both commands will open matching help files in RStudio’s help panel in the lower right corner. You can also use the help panel to search help directly, as seen in the screenshot.\nRStudio help panel. When typing a word in the search field, it will show related suggestions.Automatic code completion\nWhen you write code in RStudio, you can use its automatic code completion to remind yourself of a function’s name or arguments. Start typing the function name and pay attention to the suggestions that pop up. Use the up and down arrow to select a suggested code completion and Tab to apply it. You can also use code completion to complete function’s argument names, object, names and file names. It even works if you don’t get the spelling 100% correct.\nPackage vignettes and cheat sheets\nIn addition to the documentation for individual functions, many packages have vignettes – instructions for how to use the package to do certain tasks. Vignettes are great for learning by example. Vignettes are accessible via the package help and by using the function browseVignettes().\nThere is also a Help menu at the top of the RStudio window, that has cheat sheets for popular packages, RStudio keyboard shortcuts, and more.\nFinding more functions and packages\nRStudio’s help only searches the packages that you have installed on your machine, but there are many more available on CRAN and GitHub. To search across all available R packages, you can use the website rdocumentation.org. Often, a generic Google or internet search “R <task>” will send you to the appropriate package documentation or a forum where someone else has already asked your question. Many packages also have websites with additional help, tutorials, news and more (for example tidyverse.org).\nDealing with error messages\nDon’t get discouraged if your code doesn’t run immediately! Error messages are common when programming, and fixing errors is part of any programmers daily work. Often, the problem is a small typo in a variable name or a missing parenthesis. Watch for the red x’s next to your code in Rstudio. These may provide helpful hints about the source of the problem.\nRStudio shows a red x next to a line of code that R doesn’t understand.If you can’t fix an error yourself, start by googling it. Some error messages are too generic to diagnose a problem (e.g. “subscript out of bounds”). In that case it might help to include the name of the function or package you’re using in your query.\nAsking for help\nIf your Google search is unsuccessful, you may want to ask other R users for help. There are different places where you can ask for help. During this workshop, don’t hesitate to talk to your neighbor, compare your answers, and ask for help. You might also be interested in organizing regular meetings following the workshop to keep learning from each other. If you have a friend or colleague with more experience than you, they might also be able and willing to help you.\nBesides that, there are a few places on the internet that provide help:\nStack Overflow: Many questions have already been answered, but the challenge is to use the right words in your search to find them. If your question hasn’t been answered before and is well crafted, chances are you will get an answer in less than 5 min. Remember to follow their guidelines on how to ask a good question.\nThe R-help mailing list: it is used by a lot of people (including most of the R core team). If your question is valid (read its Posting Guide), you are likely to get an answer very fast, but the tone can be pretty dry and it is not always very welcoming to new users.\nIf your question is about a specific package rather than a base R function, see if there is a mailing list for the package. Usually it’s included in the DESCRIPTION file of the package that can be accessed using packageDescription(\"<package-name>\").\nYou can also try to contact the package author directly, by emailing them or opening an issue on the code repository (e.g., on GitHub).\nThere are also some topic-specific mailing lists (GIS, phylogenetics, etc…), the complete list is here.\nThe key to receiving help from someone is for them to rapidly grasp your problem. Thus, you should be as precise as possible when describing your problem and make it easy to pinpoint where the issue might be. Try to…\nUse the correct words to describe your problem. Otherwise you might get an answer pointing to the misuse of your words rather than answering your question.\nGeneralize what you are trying to do, so people outside your field can understand the question.\nReduce what doesn’t work to a simple reproducible example. For instance, instead of using your real data set, create a small generic one. For more information on how to write a reproducible example see this article by Hadley Wickham. The reprex package is also very helpful for this.\nInclude the output of sessionInfo() in your question. It provides information about your platform, the versions of R and the packages that you are using. As an example, here you can see the versions of R and all the packages that we are using to run the code in this lesson:\n\n\n\n\n\nsessionInfo()\n\n\n#> R version 4.0.3 (2020-10-10)\n#> Platform: x86_64-pc-linux-gnu (64-bit)\n#> Running under: Ubuntu 18.04.5 LTS\n#> \n#> Matrix products: default\n#> BLAS:   /usr/lib/x86_64-linux-gnu/openblas/libblas.so.3\n#> LAPACK: /usr/lib/x86_64-linux-gnu/libopenblasp-r0.2.20.so\n#> \n#> locale:\n#>  [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C          \n#>  [3] LC_TIME=C.UTF-8        LC_COLLATE=C.UTF-8    \n#>  [5] LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n#>  [7] LC_PAPER=C.UTF-8       LC_NAME=C             \n#>  [9] LC_ADDRESS=C           LC_TELEPHONE=C        \n#> [11] LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets  methods  \n#> [7] base     \n#> \n#> other attached packages:\n#>  [1] RSQLite_2.2.1   forcats_0.5.0   stringr_1.4.0   dplyr_1.0.2    \n#>  [5] purrr_0.3.4     readr_1.4.0     tidyr_1.1.2     tibble_3.0.4   \n#>  [9] ggplot2_3.3.2   tidyverse_1.3.0 knitr_1.30     \n#> \n#> loaded via a namespace (and not attached):\n#>  [1] tidyselect_1.1.0 xfun_0.18        haven_2.3.1     \n#>  [4] colorspace_1.4-1 vctrs_0.3.4      generics_0.0.2  \n#>  [7] htmltools_0.5.0  yaml_2.2.1       blob_1.2.1      \n#> [10] rlang_0.4.8      pillar_1.4.6     glue_1.4.2      \n#> [13] withr_2.3.0      DBI_1.1.0        bit64_4.0.5     \n#> [16] dbplyr_1.4.4     modelr_0.1.8     readxl_1.3.1    \n#> [19] lifecycle_0.2.0  munsell_0.5.0    gtable_0.3.0    \n#> [22] cellranger_1.1.0 rvest_0.3.6      memoise_1.1.0   \n#> [25] evaluate_0.14    ps_1.4.0         fansi_0.4.1     \n#> [28] broom_0.7.2      Rcpp_1.0.5       scales_1.1.1    \n#> [31] backports_1.1.10 jsonlite_1.7.1   bit_4.0.4       \n#> [34] fs_1.5.0         distill_1.0      hms_0.5.3       \n#> [37] digest_0.6.27    stringi_1.5.3    grid_4.0.3      \n#> [40] cli_2.1.0        tools_4.0.3      magrittr_1.5    \n#> [43] crayon_1.3.4     pkgconfig_2.0.3  downlit_0.2.0   \n#> [46] ellipsis_0.3.1   xml2_1.3.2       reprex_0.3.0    \n#> [49] lubridate_1.7.9  assertthat_0.2.1 rmarkdown_2.5   \n#> [52] httr_1.4.2       rstudioapi_0.11  R6_2.5.0        \n#> [55] compiler_4.0.3\n\n\n\n\nHow to learn more after the workshop?\nThe material we cover during this workshop will give you a taste of how you can use R to analyze data for your own research. However, to do advanced operations such as cleaning your dataset, using statistical methods, or creating beautiful graphics you will need to learn more.\nThe best way to become proficient and efficient at R, as with any other tool, is to use it to address your actual research questions. As a beginner, it can feel daunting to have to write a script from scratch, and given that many people make their code available online, modifying existing code to suit your purpose might make it easier for you to get started.\n\n\n\nMore resources\nMore about R\nThe Introduction to R can also be dense for people with little programming experience but it is a good place to understand the underpinnings of the R language.\nThe R FAQ is dense and technical but it is full of useful information.\nTo stay up to date, follow #rstats on twitter. Twitter can also be a way to get questions answered and learn about useful R packages and tipps (e.g., [@RLangTips])\nHow to ask good programming questions?\nThe rOpenSci community call “How to ask questions so they get answered”, (rOpenSci site and video recording) includes a presentation of the reprex package and of its philosophy.\nblog.Revolutionanalytics.com and codeblog.jonskeet.uk provide advice on how to ask programming questions.\nThis blog post by Jon Skeet has comprehensive advice on how to ask programming questions.\n\n\n\n",
      "last_modified": "2020-10-29T19:13:48+00:00"
    },
    {
      "path": "01-intro-to-r.html",
      "title": "Introduction to R",
      "author": [],
      "contents": "\n\nContents\nCreating objects in R\nSaving your code\nComments\nFunctions and their arguments\nVectors and data typesSubsetting vectors\nConditional subsetting\n\nMissing data\n\n\n\n\n\nLearning Objectives\nDefine the following terms as they relate to R: object, assign, call, function, arguments, options.\nCreate objects and assign values to them in R.\nLearn how to name objects.\nSave a script file for later use.\nUse comments to inform script.\nSolve simple arithmetic operations in R.\nCall functions and use arguments to change their default options.\nInspect the content of vectors and manipulate their content.\nSubset and extract values from vectors.\nAnalyze vectors with missing data.\n\nCreating objects in R\n\n\n\nYou can get output from R simply by typing math in the console:\n\n\n3 + 5\n12 / 7\n\n\n\nHowever, to do useful and interesting things, we need to assign values to objects. To create an object, we need to give it a name followed by the assignment operator <-, and the value we want to give it:\n\n\nweight_kg <- 55\n\n\n\n<- is the assignment operator. It assigns values on the right to objects on the left. So, after executing x <- 3, the value of x is 3. The arrow also looks like a mouth (with tongue), which makes it easy to pronounce as x eats 3. For historical reasons, you can also use = for assignments, but not in every context. Because of the slight differences in syntax, it is good practice to always use <- for assignments.\nIn RStudio, typing Alt + - (push Alt at the same time as the - key) will write <- in a single keystroke in a PC, while typing Option + - (push Option at the same time as the - key) does the same in a Mac.\nObjects can be given almost any name such as x, current_temperature, or subject_id. Here are some further guidelines on naming objects:\nYou want your object names to be explicit and not too long.\nThey cannot start with a number (2x is not valid, but x2 is).\nR is case sensitive, so for example, weight_kg is different from Weight_kg.\nThere are some names that cannot be used because they are the names of fundamental functions in R (e.g., if, else, for, see here for a complete list). In general, even if it’s allowed, it’s best to not use other function names (e.g., c, T, mean, data, df, weights). If in doubt, check the help to see if the name is already in use.\nIt’s best to avoid dots (.) within names. Many function names in R itself have them and dots also have a special meaning (methods) in R and other programming languages. To avoid confusion, don’t include dots in names.\nIt is recommended to use nouns for object names and verbs for function names.\nBe consistent in the styling of your code, such as where you put spaces, how you name objects, etc. Using a consistent coding style makes your code clearer to read for your future self and your collaborators. In R, three popular style guides come from Google, Jean Fan and the tidyverse. The tidyverse style is very comprehensive and may seem overwhelming at first. You can install the lintr package to automatically check for issues in the styling of your code.\nObjects vs. variables\nWhat are known as objects in R are known as variables in many other programming languages. Depending on the context, object and variable can have drastically different meanings. However, in this lesson, the two words are used synonymously. For more information see here.\nWhen assigning a value to an object, R does not print anything. You can force R to print the value by using parentheses or by typing the object name:\n\n\nweight_kg <- 55    # doesn't print anything\n(weight_kg <- 55)  # but putting parenthesis around the call prints the value of `weight_kg`\nweight_kg          # and so does typing the name of the object\n\n\n\nNow that R has weight_kg in memory, we can do arithmetic with it. For instance, we may want to convert this weight into pounds (weight in pounds is 2.2 times the weight in kg):\n\n\n2.2 * weight_kg\n\n\n\nWe can also change an object’s value by assigning it a new one:\n\n\nweight_kg <- 57.5\n2.2 * weight_kg\n\n\n\nThis means that assigning a value to one object does not change the values of other objects For example, let’s store the animal’s weight in pounds in a new object, weight_lb:\n\n\nweight_lb <- 2.2 * weight_kg\n\n\n\nand then change weight_kg to 100.\n\n\nweight_kg <- 100\n\n\n\nWhat do you think is the current content of the object weight_lb? 126.5 or 220?\nSaving your code\nUp to now, your code has been in the console. This is useful for quick queries but not so helpful if you want to revisit your work for any reason. A script can be opened by pressing Ctrl + Shift + N. It it wise to save your script file immediately. To do this press Ctrl + S. This will open a dialogue box where you can decide where to save your script file, and what to name it. The .R file extension is added automatically and ensures your file will open with RStudio.\nDon’t forget to save your work periodically by pressing Ctrl + S.\nComments\nThe comment character in R is #, anything to the right of a # in a script will be ignored by R. It is useful to leave notes and explanations in your scripts. RStudio makes it easy to comment or uncomment a paragraph: after selecting the lines you want to comment, press at the same time on your keyboard Ctrl + Shift + C. If you only want to comment out one line, you can put the cursor at any location of that line (i.e. no need to select the whole line), then press Ctrl + Shift + C.\n\nChallenge\nWhat are the values after each statement in the following?\n\n\nmass <- 47.5            # mass?\nage  <- 122             # age?\nmass <- mass * 2.0      # mass?\nage  <- age - 20        # age?\nmass_index <- mass/age  # mass_index?\n\n\n\n\n\n\n\nFunctions and their arguments\nFunctions are “canned scripts” that automate more complicated sets of commands including operations assignments, etc. Many functions are predefined, or can be made available by importing R packages (more on that later). A function usually takes one or more inputs called arguments. Functions often (but not always) return a value. A typical example would be the function sqrt(). The input (the argument) must be a number, and the return value (in fact, the output) is the square root of that number. Executing a function (‘running it’) is called calling the function. An example of a function call is:\n\n\nweight_kg <- sqrt(10)\n\n\n\nHere, the value of 10 is given to the sqrt() function, the sqrt() function calculates the square root, and returns the value which is then assigned to the object weight_kg. This function is very simple, because it takes just one argument.\nThe return ‘value’ of a function need not be numerical (like that of sqrt()), and it also does not need to be a single item: it can be a set of things, or even a dataset. We’ll see that when we read data files into R.\nArguments can be anything, not only numbers or filenames, but also other objects. Exactly what each argument means differs per function, and must be looked up in the documentation (see below). Some functions take arguments which may either be specified by the user, or, if left out, take on a default value: these are called options. Options are typically used to alter the way the function operates, such as whether it ignores ‘bad values’, or what symbol to use in a plot. However, if you want something specific, you can specify a value of your choice which will be used instead of the default.\nLet’s try a function that can take multiple arguments: round().\n\n\nround(3.14159)\n\n\n#> [1] 3\n\nHere, we’ve called round() with just one argument, 3.14159, and it has returned the value 3. That’s because the default is to round to the nearest whole number. If we want more digits we can see how to do that by getting information about the round function. We can use args(round) to find what arguments it takes, or look at the help for this function using ?round.\n\n\nargs(round)\n\n\n#> function (x, digits = 0) \n#> NULL\n\n\n\n?round\n\n\n\nWe see that if we want a different number of digits, we can type digits = 2 or however many we want.\n\n\nround(3.14159, digits = 2)\n\n\n#> [1] 3.14\n\nIf you provide the arguments in the exact same order as they are defined you don’t have to name them:\n\n\nround(3.14159, 2)\n\n\n#> [1] 3.14\n\nAnd if you do name the arguments, you can switch their order:\n\n\nround(digits = 2, x = 3.14159)\n\n\n#> [1] 3.14\n\nIt’s good practice to put the non-optional arguments (like the number you’re rounding) first in your function call, and to then specify the names of all optional arguments. If you don’t, someone reading your code might have to look up the definition of a function with unfamiliar arguments to understand what you’re doing.\nVectors and data types\n\n\n\nA vector is the most common and basic data type in R, and is pretty much the workhorse of R. A vector is composed by a series of values, which can be either numbers or characters. We can assign a series of values to a vector using the c() function. For example we can create a vector of animal weights and assign it to a new object weight_g:\n\n\nweight_g <- c(50, 60, 65, 82)\nweight_g\n\n\n\nA vector can also contain characters:\n\n\nanimals <- c(\"mouse\", \"rat\", \"dog\")\nanimals\n\n\n\nThe quotes around “mouse”, “rat”, etc. are essential here. Without the quotes R will assume objects have been created called mouse, rat and dog. As these objects don’t exist in R’s memory, there will be an error message.\nThere are many functions that allow you to inspect the content of a vector. length() tells you how many elements are in a particular vector:\n\n\nlength(weight_g)\nlength(animals)\n\n\n\nAn important feature of a vector, is that all of the elements are the same type of data. The function class() indicates what kind of object you are working with:\n\n\nclass(weight_g)\nclass(animals)\n\n\n\nThe function str() provides an overview of the structure of an object and its elements. It is a useful function when working with large and complex objects:\n\n\nstr(weight_g)\nstr(animals)\n\n\n\nYou can use the c() function to add other elements to your vector:\n\n\nweight_g <- c(weight_g, 90) # add to the end of the vector\nweight_g <- c(30, weight_g) # add to the beginning of the vector\nweight_g\n\n\n\nIn the first line, we take the original vector weight_g, add the value 90 to the end of it, and save the result back into weight_g. Then we add the value 30 to the beginning, again saving the result back into weight_g.\nWe can do this over and over again to grow a vector, or assemble a dataset. As we program, this may be useful to add results that we are collecting or calculating.\nAn atomic vector is the simplest R data type and is a linear vector of a single type. Above, we saw 2 of the 6 main atomic vector types that R uses: \"character\" and \"numeric\" (or \"double\"). These are the basic building blocks that all R objects are built from. The other 4 atomic vector types are:\n\"logical\" for TRUE and FALSE (the boolean data type)\n\"integer\" for integer numbers (e.g., 2L, the L indicates to R that it’s an integer)\n\"complex\" to represent complex numbers with real and imaginary parts (e.g., 1 + 4i) and that’s all we’re going to say about them\n\"raw\" for bitstreams that we won’t discuss further\nYou can check the type of your vector using the typeof() function and inputting your vector as the argument.\nVectors are one of the many data structures that R uses. Other important ones are lists (list), matrices (matrix), data frames (data.frame), factors (factor) and arrays (array).\n\nChallenge\nWe’ve seen that atomic vectors can be of type character, numeric (or double), integer, and logical. But what happens if we try to mix these types in a single vector?\n\nAnswer\nR implicitly converts them to all be the same type\n\n\nWhat will happen in each of these examples? (hint: use class() to check the data type of your objects):\n\n\nnum_char <- c(1, 2, 3, \"a\")\nnum_logical <- c(1, 2, 3, TRUE)\nchar_logical <- c(\"a\", \"b\", \"c\", TRUE)\ntricky <- c(1, 2, 3, \"4\")\n\n\n\nWhy do you think it happens?\n\nAnswer\nVectors can be of only one data type. R tries to convert (coerce) the content of this vector to find a “common denominator” that doesn’t lose any information.\n\n\nHow many values in combined_logical are \"TRUE\" (as a character) in the following example (reusing the 2 ..._logicals from above):\n\n\ncombined_logical <- c(num_logical, char_logical)\n\n\n\n\nAnswer\nOnly one. There is no memory of past data types, and the coercion happens the first time the vector is evaluated. Therefore, the TRUE in num_logical gets converted into a 1 before it gets converted into \"1\" in combined_logical.\n\n\nYou’ve probably noticed that objects of different types get converted into a single, shared type within a vector. In R, we call converting objects from one class into another class coercion. These conversions happen according to a hierarchy, whereby some types get preferentially coerced into other types. Can you draw a diagram that represents the hierarchy of how these data types are coerced?\n\nAnswer\nlogical → numeric → character ← logical\n\n\n\n\n\n\nSubsetting vectors\nIf we want to extract one or several values from a vector, we must provide one or several indices in square brackets. For instance:\n\n\nanimals <- c(\"mouse\", \"rat\", \"dog\", \"cat\")\nanimals[2]\n\n\n#> [1] \"rat\"\n\nanimals[c(3, 2)]\n\n\n#> [1] \"dog\" \"rat\"\n\nWe can also repeat the indices to create an object with more elements than the original one:\n\n\nmore_animals <- animals[c(1, 2, 3, 2, 1, 4)]\nmore_animals\n\n\n#> [1] \"mouse\" \"rat\"   \"dog\"   \"rat\"   \"mouse\" \"cat\"\n\nR indices start at 1. Programming languages like Fortran, MATLAB, Julia, and R start counting at 1, because that’s what human beings typically do. Languages in the C family (including C++, Java, Perl, and Python) count from 0 because that’s simpler for computers to do.\nConditional subsetting\nAnother common way of subsetting is by using a logical vector. TRUE will select the element with the same index, while FALSE will not:\n\n\nweight_g <- c(21, 34, 39, 54, 55)\nweight_g[c(TRUE, FALSE, FALSE, TRUE, TRUE)]\n\n\n#> [1] 21 54 55\n\nTypically, these logical vectors are not typed by hand, but are the output of other functions or logical tests. For instance, if you wanted to select only the values above 50:\n\n\nweight_g > 50    # will return logicals with TRUE for the indices that meet the condition\n\n\n#> [1] FALSE FALSE FALSE  TRUE  TRUE\n\n## so we can use this to select only the values above 50\nweight_g[weight_g > 50]\n\n\n#> [1] 54 55\n\nYou can combine multiple tests using & (both conditions are true, AND) or | (at least one of the conditions is true, OR):\n\n\nweight_g[weight_g > 30 & weight_g < 50]\n\n\n#> [1] 34 39\n\nweight_g[weight_g <= 30 | weight_g == 55]\n\n\n#> [1] 21 55\n\nweight_g[weight_g >= 30 & weight_g == 21]\n\n\n#> numeric(0)\n\nHere, > for “greater than”, < stands for “less than”, <= for “less than or equal to”, and == for “equal to”. The double equal sign == is a test for numerical equality between the left and right hand sides, and should not be confused with the single = sign, which performs variable assignment (similar to <-).\nA common task is to search for certain strings in a vector. One could use the “or” operator | to test for equality to multiple values, but this can quickly become tedious. The function %in% allows you to test if any of the elements of a search vector are found:\n\n\nanimals <- c(\"mouse\", \"rat\", \"dog\", \"cat\", \"cat\")\n\n# return both rat and cat\nanimals[animals == \"cat\" | animals == \"rat\"] \n\n\n#> [1] \"rat\" \"cat\" \"cat\"\n\n# return a logical vector that is TRUE for the elements within animals\n# that are found in the character vector and FALSE for those that are not\nanimals %in% c(\"rat\", \"cat\", \"dog\", \"duck\", \"goat\") \n\n\n#> [1] FALSE  TRUE  TRUE  TRUE  TRUE\n\n# use the logical vector created by %in% to return elements from animals \n# that are found in the character vector\nanimals[animals %in% c(\"rat\", \"cat\", \"dog\", \"duck\", \"goat\")]\n\n\n#> [1] \"rat\" \"dog\" \"cat\" \"cat\"\n\n\nChallenge (optional)\nCan you figure out why \"four\" > \"five\" returns TRUE?\n\nAnswer\nWhen using “>” or “<” on strings, R compares their alphabetical order. Here “four” comes after “five”, and therefore is “greater than” it.\n\n\n\n\n\n\nMissing data\nAs R was designed to analyze datasets, it includes the concept of missing data (which is uncommon in other programming languages). Missing data are represented in vectors as NA.\nWhen doing operations on numbers, most functions will return NA if the data you are working with include missing values. This feature makes it harder to overlook the cases where you are dealing with missing data. You can add the argument na.rm = TRUE to calculate the result as if the missing values were removed (rm stands for ReMoved) first.\n\n\nheights <- c(2, 4, 4, NA, 6)\nmean(heights)\nmax(heights)\nmean(heights, na.rm = TRUE)\nmax(heights, na.rm = TRUE)\n\n\n\nIf your data include missing values, you may want to become familiar with the functions is.na(), na.omit(), and complete.cases(). See below for examples.\n\n\n## Extract those elements which are not missing values.\nheights[!is.na(heights)]\n\n## Returns the object with incomplete cases removed. \n#The returned object is an atomic vector of type `\"numeric\"` (or #`\"double\"`).\nna.omit(heights)\n\n## Extract those elements which are complete cases. \n#The returned object is an atomic vector of type `\"numeric\"` (or #`\"double\"`).\nheights[complete.cases(heights)]\n\n\n\nRecall that you can use the typeof() function to find the type of your atomic vector.\n\nChallenge\nUsing this vector of heights in inches, create a new vector, heights_no_na, with the NAs removed.\n\n\nheights <- c(63, 69, 60, 65, NA, 68, 61, 70, 61, 59, 64, 69, 63, 63, NA, 72, 65, 64, 70, 63, 65)\n\n\n\nUse the function median() to calculate the median of the heights vector.\nUse R to figure out how many people in the set are taller than 67 inches.\n\nAnswer\n\nheights <- c(63, 69, 60, 65, NA, 68, 61, 70, 61, 59, 64, 69, 63, 63, NA, 72, 65, \n             64, 70, 63, 65)\n\n# 1.\nheights_no_na <- heights[!is.na(heights)] \n# or\nheights_no_na <- na.omit(heights)\n# or\nheights_no_na <- heights[complete.cases(heights)]\n# 2.\nmedian(heights, na.rm = TRUE)\n# 3.\nheights_above_67 <- heights_no_na[heights_no_na > 67]\nlength(heights_above_67)\n\n\n\n\n\n\n\n\nNow that we have learned how to write scripts, and the basics of R’s data structures, we are ready to start working with the Portal dataset we have been using in the other lessons, and learn about data frames.\n\n\n\n",
      "last_modified": "2020-10-29T19:13:49+00:00"
    },
    {
      "path": "02-starting-with-data.html",
      "title": "Starting with data",
      "author": [],
      "contents": "\n\nContents\nLoading the survey dataDownloading the data\nInstalling the tidyverse package\nReading the data into R\nInspecting the data\n\nData FramesWhat are data frames?\nInspecting data frames\nIndexing and subsetting data frames\n\nFactorsConverting factors\nRenaming factors\n\nFormatting dates\n\n\n\n\n\nLearning Objectives\nLoad external data from a .csv file into a data frame.\nDescribe what a data frame is.\nSummarize the contents of a data frame.\nUse indexing to subset specific portions of data frames.\nDescribe what a factor is.\nConvert between strings and factors.\nReorder and rename factors.\nChange how character strings are handled in a data frame.\nFormat dates.\n\nLoading the survey data\n\n\n\nWe are investigating the animal species diversity and weights found within plots at our study site. The dataset is stored as a comma separated value (CSV) file. Each row holds information for a single animal, and the columns represent:\nColumn\nDescription\nrecord_id\nUnique id for the observation\nmonth\nmonth of observation\nday\nday of observation\nyear\nyear of observation\nplot_id\nID of a particular plot\nspecies_id\n2-letter code\nsex\nsex of animal (“M”, “F”)\nhindfoot_length\nlength of the hindfoot in mm\nweight\nweight of the animal in grams\ngenus\ngenus of animal\nspecies\nspecies of animal\ntaxon\ne.g. Rodent, Reptile, Bird, Rabbit\nplot_type\ntype of plot\nDownloading the data\nWe are going to use the R function download.file() to download the CSV file that contains the survey data from Figshare, and we will use read_csv() to load the content of the CSV file into R.\nInside the download.file command, the first entry is a character string with the source URL (“https://ndownloader.figshare.com/files/2292169”). This source URL downloads a CSV file from figshare. The text after the comma (“data_raw/portal_data_joined.csv”) is the destination of the file on your local machine. You’ll need to have a folder on your machine called “data_raw” where you’ll download the file. So this command downloads a file from Figshare, names it “portal_data_joined.csv” and adds it to a preexisting folder named “data_raw”.\n\n\ndownload.file(url = \"https://ndownloader.figshare.com/files/2292169\",\n              destfile = \"data_raw/portal_data_joined.csv\")\n\n\n\nInstalling the tidyverse package\nThe file has now been downloaded to the destination you specified, but R has not yet loaded the data from the file into memory. To do this, we can use the read_csv() function from the tidyverse package.\nPackages in R are basically sets of additional functions that let you do more stuff. The functions we’ve been using so far, like round(), sqrt(), or c(), come built into R; packages give you access to additional functions. Before you use a package for the first time you need to install it on your machine, and then you should import it in every subsequent R session when you need it.\nTo install the tidyverse package, we can type install.packages(\"tidyverse\") straight into the console. In fact, it’s better to write this in the console than in our script for any package, as there’s no need to re-install packages every time we run the script. Then, to load the package type:\n\n\n## load the tidyverse packages, incl. dplyr\nlibrary(tidyverse)\n\n\n\nReading the data into R\nNow we can use the functions from the tidyverse package. Let’s use read_csv() to read the data into a data frame (we will learn more about data frames later):\n\n\nsurveys <- read_csv(\"data_raw/portal_data_joined.csv\")\n\n\n\nYou will see the message Parsed with column specification, followed by each column name and its data type. When you execute read_csv on a data file, it looks through the first 1000 rows of each column and guesses its data type. For example, in this dataset, read_csv() reads weight as col_double (a numeric data type), and species as col_character. You have the option to specify the data type for a column manually by using the col_types argument in read_csv.\nread_csv() assumes that fields are delineated by commas, however, in several countries, the comma is used as a decimal separator and the semicolon (;) is used as a field delineator. If you want to read in this type of files in R, you can use the read_csv2() function. There is also the read_tsv() for tab separated data files and read_delim() for less common formats. Check out the help for read_csv() by typing ?read_csv to learn more.\nKnowing the parameters of your data files will ease up the import step. It is good to develop the habits of looking at and recording parameters of your csv files such as the character encoding, control characters used for line ending, date format (if the date is not splitted into three variables), and the presence of unexpected newlines.\nInspecting the data\nWe can see the contents of the first few lines of the data by typing its name: surveys. By default, this will show show you as many rows and columns of the data as fit on your screen. If you wanted to the first 50 rows, you could type print(surveys, n = 50)\nWe can also extract the first few lines of this data using the function head():\n\n\nhead(surveys)\n\n\n#> # A tibble: 6 x 13\n#>   record_id month   day  year plot_id species_id sex   hindfoot_length\n#>       <dbl> <dbl> <dbl> <dbl>   <dbl> <chr>      <chr>           <dbl>\n#> 1         1     7    16  1977       2 NL         M                  32\n#> 2        72     8    19  1977       2 NL         M                  31\n#> 3       224     9    13  1977       2 NL         <NA>               NA\n#> 4       266    10    16  1977       2 NL         <NA>               NA\n#> 5       349    11    12  1977       2 NL         <NA>               NA\n#> 6       363    11    12  1977       2 NL         <NA>               NA\n#> # … with 5 more variables: weight <dbl>, genus <chr>, species <chr>,\n#> #   taxa <chr>, plot_type <chr>\n\nUnlike the print() function, head() returns the extracted data. You could use it to assign the first 100 rows of surveys to an object using surveys_sample <- head(surveys, 100). This can be useful if you want to try out complex computations on a subset of your data before you apply them to the whole data set. There is a similar function that lets you extract the last few lines of the data set. It is called (you might have guessed it) tail().\nTo open the dataset in RStudio’s Data Viewer, use the view() function:\n\n\nview(surveys)\n\n\n\nData Frames\nWhat are data frames?\nWhen we loaded the data into R, it got stored as an object of class tibble, which is a special kind of data frame (the difference is not important for our purposes, but you can learn more about tibbles here). Data frames are the de facto data structure for most tabular data, and what we use for statistics and plotting. Data frames can be created by hand, but most commonly they are generated by functions like read_csv(); in other words, when importing spreadsheets from your hard drive or the web.\nA data frame is the representation of data in the format of a table where the columns are vectors that all have the same length. Because columns are vectors, each column must contain a single type of data (e.g., characters, integers, factors). For example, here is a figure depicting a data frame comprising a numeric, a character, and a logical vector.\n We can see this also when inspecting the structure of a data frame with the function str():\n\n\nstr(surveys)\n\n\n\nInspecting data frames\nWe already saw how the functions head() and str() can be useful to check the content and the structure of a data frame. Here is a non-exhaustive list of functions to get a sense of the content/structure of the data. Let’s try them out!\nSize:\ndim(surveys) - returns a vector with the number of rows in the first element, and the number of columns as the second element (the dimensions of the object)\nnrow(surveys) - returns the number of rows\nncol(surveys) - returns the number of columns\n\nContent:\nhead(surveys) - shows the first 6 rows\ntail(surveys) - shows the last 6 rows\n\nNames:\nnames(surveys) - returns the column names (synonym of colnames() for data.frame objects)\nrownames(surveys) - returns the row names\n\nSummary:\nstr(surveys) - structure of the object and information about the class, length and content of each column\nsummary(surveys) - summary statistics for each column\n\nNote: most of these functions are “generic”, they can be used on other types of objects besides data.frame.\n\nChallenge\nBased on the output of str(surveys), can you answer the following questions?\nWhat is the class of the object surveys?\nHow many rows and how many columns are in this object?\n\nAnswer\n\nstr(surveys)\n\n\n#> tibble [34,786 × 13] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n#>  $ record_id      : num [1:34786] 1 72 224 266 349 363 435 506 588 661 ...\n#>  $ month          : num [1:34786] 7 8 9 10 11 11 12 1 2 3 ...\n#>  $ day            : num [1:34786] 16 19 13 16 12 12 10 8 18 11 ...\n#>  $ year           : num [1:34786] 1977 1977 1977 1977 1977 ...\n#>  $ plot_id        : num [1:34786] 2 2 2 2 2 2 2 2 2 2 ...\n#>  $ species_id     : chr [1:34786] \"NL\" \"NL\" \"NL\" \"NL\" ...\n#>  $ sex            : chr [1:34786] \"M\" \"M\" NA NA ...\n#>  $ hindfoot_length: num [1:34786] 32 31 NA NA NA NA NA NA NA NA ...\n#>  $ weight         : num [1:34786] NA NA NA NA NA NA NA NA 218 NA ...\n#>  $ genus          : chr [1:34786] \"Neotoma\" \"Neotoma\" \"Neotoma\" \"Neotoma\" ...\n#>  $ species        : chr [1:34786] \"albigula\" \"albigula\" \"albigula\" \"albigula\" ...\n#>  $ taxa           : chr [1:34786] \"Rodent\" \"Rodent\" \"Rodent\" \"Rodent\" ...\n#>  $ plot_type      : chr [1:34786] \"Control\" \"Control\" \"Control\" \"Control\" ...\n#>  - attr(*, \"spec\")=\n#>   .. cols(\n#>   ..   record_id = col_double(),\n#>   ..   month = col_double(),\n#>   ..   day = col_double(),\n#>   ..   year = col_double(),\n#>   ..   plot_id = col_double(),\n#>   ..   species_id = col_character(),\n#>   ..   sex = col_character(),\n#>   ..   hindfoot_length = col_double(),\n#>   ..   weight = col_double(),\n#>   ..   genus = col_character(),\n#>   ..   species = col_character(),\n#>   ..   taxa = col_character(),\n#>   ..   plot_type = col_character()\n#>   .. )\n\n## * class: data frame\n## * how many rows: 34786,  how many columns: 13\n\n\n\n\n\n\n\n\nIndexing and subsetting data frames\n\n\n\nOur survey data frame has rows and columns (it has 2 dimensions), if we want to extract some specific data from it, we need to specify the “coordinates” we want from it. Row numbers come first, followed by column numbers. However, note that different ways of specifying these coordinates lead to results with different classes.\n\n\n# first element in the first column of the data frame (as a vector)\nsurveys[1, 1]   \n# first element in the 6th column (as a vector)\nsurveys[1, 6]   \n# first column of the data frame (as a vector)\nsurveys[, 1]    \n# first column of the data frame (as a data.frame)\nsurveys[1]      \n# first three rows of the 6th column (as a vector)\nsurveys[1:3, 6] \n# the 3rd row of the data frame (as a data.frame)\nsurveys[3, ]    \n# equivalent to head_surveys <- head(surveys)\nhead_surveys <- surveys[1:6, ] \n\n\n\n: is a special function that creates numeric vectors of integers in increasing or decreasing order, test 1:10 and 10:1 for instance.\nYou can also exclude certain indices of a data frame using the “-” sign:\n\n\nsurveys[, -1]          # The whole data frame, except the first column\nsurveys[-(7:34786), ] # Equivalent to head(surveys)\n\n\n\nData frames can be subset by calling indices (as shown previously), but also by calling their column names directly:\n\n\nsurveys[\"species_id\"]       # Result is a data.frame\nsurveys[, \"species_id\"]     # Result is a vector\nsurveys[[\"species_id\"]]     # Result is a vector\nsurveys$species_id          # Result is a vector\n\n\n\nIn RStudio, you can use the autocompletion feature to get the full and correct names of the columns.\n\nChallenge\nCreate a data.frame (surveys_200) containing only the data in row 200 of the surveys dataset.\nNotice how nrow() gave you the number of rows in a data.frame?\nUse that number to pull out just that last row in the data frame.\nCompare that with what you see as the last row using tail() to make sure it’s meeting expectations.\nPull out that last row using nrow() instead of the row number.\nCreate a new data frame (surveys_last) from that last row.\n\nUse nrow() to extract the row that is in the middle of the data frame. Store the content of this row in an object named surveys_middle.\nCombine nrow() with the - notation above to reproduce the behavior of head(surveys), keeping just the first through 6th rows of the surveys dataset.\n\nAnswer\n\n## 1.\nsurveys_200 <- surveys[200, ]\n## 2.\n# Saving `n_rows` to improve readability and reduce duplication\nn_rows <- nrow(surveys)\nsurveys_last <- surveys[n_rows, ]\n## 3.\nsurveys_middle <- surveys[n_rows / 2, ]\n## 4.\nsurveys_head <- surveys[-(7:n_rows), ]\n\n\n\n\n\n\n\n\nFactors\n\n\n\nWhen we did str(surveys) we saw that several of the columns consist of integers. The columns genus, species, sex, plot_type, … however, are of the class character. Arguably, these columns contain categorical data, that is, they can only take on a limited number of values.\nR has a special class for working with categorical data, called factor. Factors are very useful and actually contribute to making R particularly well suited to working with data. So we are going to spend a little time introducing them.\nOnce created, factors can only contain a pre-defined set of values, known as levels. Factors are stored as integers associated with labels and they can be ordered or unordered. While factors look (and often behave) like character vectors, they are actually treated as integer vectors by R. So you need to be very careful when treating them as strings.\nWhen importing a data frame with read_csv(), the columns that contain text are not automatically coerced (=converted) into the factor data type, but once we have loaded the data we can do the conversion using the factor() function:\n\n\nsurveys$sex <- factor(surveys$sex)\n\n\n\nWe can see that the conversion has worked by using the summary() function again. This produces a table with the counts for each factor level:\n\n\nsummary(surveys$sex)\n\n\n\nBy default, R always sorts levels in alphabetical order. For instance, if you have a factor with 2 levels:\n\n\nsex <- factor(c(\"male\", \"female\", \"female\", \"male\"))\n\n\n\nR will assign 1 to the level \"female\" and 2 to the level \"male\" (because f comes before m, even though the first element in this vector is \"male\"). You can see this by using the function levels() and you can find the number of levels using nlevels():\n\n\nlevels(sex)\nnlevels(sex)\n\n\n\nSometimes, the order of the factors does not matter, other times you might want to specify the order because it is meaningful (e.g., “low”, “medium”, “high”), it improves your visualization, or it is required by a particular type of analysis. Here, one way to reorder our levels in the sex vector would be:\n\n\nsex # current order\n\n\n#> [1] male   female female male  \n#> Levels: female male\n\nsex <- factor(sex, levels = c(\"male\", \"female\"))\nsex # after re-ordering\n\n\n#> [1] male   female female male  \n#> Levels: male female\n\nIn R’s memory, these factors are represented by integers (1, 2, 3), but are more informative than integers because factors are self describing: \"female\", \"male\" is more descriptive than 1, 2. Which one is “male”? You wouldn’t be able to tell just from the integer data. Factors, on the other hand, have this information built in. It is particularly helpful when there are many levels (like the species names in our example dataset).\n\nChallenge\nChange the columns taxa and genus in the surveys data frame into a factor.\nUsing the functions you learned before, can you find out…\nHow many rabbits were observed?\nHow many different genera are in the genus column?\n\n\nAnswer\n\nsurveys$genus <- factor(surveys$genus)\nsummary(surveys)\nnlevels(surveys$genus)\n## * how many genera: There are 26 unique genera in the `genus` column.\n## * how many rabbts: There are 75 rabbits in the `taxa` column.\n\n\n\n\n\n\n\n\nConverting factors\nIf you need to convert a factor to a character vector, you use as.character(x).\n\n\nas.character(sex)\n\n\n\nIn some cases, you may have to convert factors where the levels appear as numbers (such as concentration levels or years) to a numeric vector. For instance, in one part of your analysis the years might need to be encoded as factors (e.g., comparing average weights across years) but in another part of your analysis they may need to be stored as numeric values (e.g., doing math operations on the years). This conversion from factor to numeric is a little trickier. The as.numeric() function returns the index values of the factor, not its levels, so it will result in an entirely new (and unwanted in this case) set of numbers. One method to avoid this is to convert factors to characters, and then to numbers.\nAnother method is to use the levels() function. Compare:\n\n\nyear_fct <- factor(c(1990, 1983, 1977, 1998, 1990))\nas.numeric(year_fct)               # Wrong! And there is no warning...\nas.numeric(as.character(year_fct)) # Works...\nas.numeric(levels(year_fct))[year_fct]    # The recommended way.\n\n\n\nNotice that in the levels() approach, three important steps occur:\nWe obtain all the factor levels using levels(year_fct)\nWe convert these levels to numeric values using as.numeric(levels(year_fct))\nWe then access these numeric values using the underlying integers of the vector year_fct inside the square brackets\nRenaming factors\nWhen your data is stored as a factor, you can use the plot() function to get a quick glance at the number of observations represented by each factor level. Let’s look at the number of males and females captured over the course of the experiment:\n\n\n## bar plot of the number of females and males captured during the experiment:\nplot(surveys$sex)\n\n\n\n\nHowver, as we saw when we used summary(surveys$sex), there are about 1700 individuals for which the sex information hasn’t been recorded. To show them in the plot, we can turn the missing values into a factor level with the addNA() function. We will also have to give the new factor level a label. We are going to work with a copy of the sex column, so we’re not modifying the working copy of the data frame:\n\n\nsex <- surveys$sex\nlevels(sex)\n\n\n#> [1] \"F\" \"M\"\n\nsex <- addNA(sex)\nlevels(sex)\n\n\n#> [1] \"F\" \"M\" NA\n\nhead(sex)\n\n\n#> [1] M    M    <NA> <NA> <NA> <NA>\n#> Levels: F M <NA>\n\nlevels(sex)[3] <- \"undetermined\"\nlevels(sex)\n\n\n#> [1] \"F\"            \"M\"            \"undetermined\"\n\nhead(sex)\n\n\n#> [1] M            M            undetermined undetermined undetermined\n#> [6] undetermined\n#> Levels: F M undetermined\n\nNow we can plot the data again, using plot(sex).\n\n\n\n\nChallenge\nRename “F” and “M” to “female” and “male” respectively.\nNow that we have renamed the factor level to “undetermined”, can you recreate the barplot such that “undetermined” is last (after “male”)?\n\nAnswer\n\nlevels(sex)[1:2] <- c(\"female\", \"male\")\nsex <- factor(sex, levels = c(\"female\", \"male\", \"undetermined\"))\nplot(sex)\n\n\n\n\n\n\n\n\nChallenge\nWe have seen how data frames are created when using read_csv(), but they can also be created by hand with the data.frame() function. There are a few mistakes in this hand-crafted data.frame. Can you spot and fix them? Don’t hesitate to experiment!\n\nanimal_data <- data.frame(\n         animal = c(dog, cat, sea cucumber, sea urchin),\n         feel = c(\"furry\", \"squishy\", \"spiny\"),\n         weight = c(45, 8 1.1, 0.8)\n         )\n\n\n\n\nCan you predict the class for each of the columns in the following example? Check your guesses using str(country_climate):\nAre they what you expected? Why? Why not?\nWhat would you need to change to ensure that each column had the accurate data type?\n\n\n\n   country_climate <- data.frame(\n          country = c(\"Canada\", \"Panama\", \"South Africa\", \"Australia\"),\n          climate = c(\"cold\", \"hot\", \"temperate\", \"hot/temperate\"),\n          temperature = c(10, 30, 18, \"15\"),\n          northern_hemisphere = c(TRUE, TRUE, FALSE, \"FALSE\"),\n          has_kangaroo = c(FALSE, FALSE, FALSE, 1)\n          )\n\n\n\n\n\n\n\nAnswer\nmissing quotations around the names of the animals\nmissing one entry in the feel column (probably for one of the furry animals)\nmissing one comma in the weight column\ncountry, climate, temperature, and northern_hemisphere are characters; has_kangaroo is numeric\nusing factor() one could replace character columns with factors columns\nremoving the quotes in temperature and northern_hemisphere and replacing 1 by TRUE in the has_kangaroo column would give what was probably intended\n\n\nThe automatic conversion of data type is sometimes a blessing, sometimes an annoyance. Be aware that it exists, learn the rules, and double check that data you import in R are of the correct type within your data frame. If not, use it to your advantage to detect mistakes that might have been introduced during data entry (for instance, a letter in a column that should only contain numbers).\nLearn more in this RStudio tutorial\nFormatting dates\nOne of the most common issues that new (and experienced!) R users have is converting date and time information into a variable that is appropriate and usable during analyses. As a reminder from earlier in this lesson, the best practice for dealing with date data is to ensure that each component of your date is stored as a separate variable. Using str(), We can confirm that our data frame has a separate column for day, month, and year, and that each contains integer values.\n\n\nstr(surveys)\n\n\n\nWe are going to use the ymd() function from the package lubridate (which belongs to the tidyverse; learn more here). lubridate gets installed as part as the tidyverse installation. When you load the tidyverse (library(tidyverse)), the core packages (the packages used in most data analyses) get loaded. lubridate however does not belong to the core tidyverse, so you have to load it explicitly with library(lubridate)\nStart by loading the required package:\n\n\nlibrary(lubridate)\n\n\n\nymd() takes a vector representing year, month, and day, and converts it to a Date vector. Date is a class of data recognized by R as being a date and can be manipulated as such. The argument that the function requires is flexible, but, as a best practice, is a character vector formatted as “YYYY-MM-DD”.\nLet’s create a date object and inspect the structure:\n\n\nmy_date <- ymd(\"2015-01-01\")\nstr(my_date)\n\n\n\nNow let’s paste the year, month, and day separately - we get the same result:\n\n\n# sep indicates the character to use to separate each component\nmy_date <- ymd(paste(\"2015\", \"1\", \"1\", sep = \"-\")) \nstr(my_date)\n\n\n\nNow we apply this function to the surveys dataset. Create a character vector from the year, month, and day columns of surveys using paste():\n\n\npaste(surveys$year, surveys$month, surveys$day, sep = \"-\")\n\n\n\nThis character vector can be used as the argument for ymd():\n\n\nymd(paste(surveys$year, surveys$month, surveys$day, sep = \"-\"))\n\n\n\nThere is a warning telling us that some dates could not be parsed (understood) by the ymd() function. For these dates, the function has returned NA, which means they are treated as missing values. We will deal with this problem later, but first we add the resulting Date vector to the surveys data frame as a new column called date:\n\n\nsurveys$date <- ymd(paste(surveys$year, surveys$month, surveys$day, sep = \"-\"))\nstr(surveys) # notice the new column, with 'date' as the class\n\n\n\nLet’s make sure everything worked correctly. One way to inspect the new column is to use summary():\n\n\nsummary(surveys$date)\n\n\n#>         Min.      1st Qu.       Median         Mean      3rd Qu. \n#> \"1977-07-16\" \"1984-03-12\" \"1990-07-22\" \"1990-12-15\" \"1997-07-29\" \n#>         Max.         NA's \n#> \"2002-12-31\"        \"129\"\n\nLet’s investigate why some dates could not be parsed.\nWe can use the functions we saw previously to deal with missing data to identify the rows in our data frame that are failing. If we combine them with what we learned about subsetting data frames earlier, we can extract the columns “year,”month“,”day\" from the records that have NA in our new column date. We will also use head() so we don’t clutter the output:\n\n\nmissing_dates <- surveys[is.na(surveys$date), c(\"year\", \"month\", \"day\")]\n\nhead(missing_dates)\n\n\n#> # A tibble: 6 x 3\n#>    year month   day\n#>   <dbl> <dbl> <dbl>\n#> 1  2000     9    31\n#> 2  2000     4    31\n#> 3  2000     4    31\n#> 4  2000     4    31\n#> 5  2000     4    31\n#> 6  2000     9    31\n\nWhy did these dates fail to parse? If you had to use these data for your analyses, how would you deal with this situation?\n\n\n\n",
      "last_modified": "2020-10-29T19:13:54+00:00"
    },
    {
      "path": "03-dplyr.html",
      "title": "Manipulating, analyzing and exporting data with tidyverse",
      "author": [],
      "contents": "\n\nContents\nData manipulation using dplyr and tidyrWhat are dplyr and tidyr?\n\nSelecting columns and filtering rows\nThe pipe %>%\nMutate\nSplit-apply-combine data analysis with group_by() and summarize()\nCounting\nReshaping with gather and spreadSpreading\nGathering\n\nExporting data\n\n\n\n\n\nLearning Objectives\nDescribe the purpose of the dplyr and tidyr packages.\nSelect certain columns in a data frame with the dplyr function select.\nSelect certain rows in a data frame according to filtering conditions with the dplyr function filter .\nLink the output of one dplyr function to the input of another function with the ‘pipe’ operator %>%.\nAdd new columns to a data frame that are functions of existing columns with mutate.\nUse the split-apply-combine concept for data analysis.\nUse summarize, group_by, and count to split a data frame into groups of observations, apply summary statistics for each group, and then combine the results.\nDescribe the concept of a wide and a long table format and for which purpose those formats are useful.\nDescribe what key-value pairs are.\nReshape a data frame from long to wide format and back with the spread and gather commands from the tidyr package.\nExport a data frame to a .csv file.\n\nData manipulation using dplyr and tidyr\nBracket subsetting is handy, but it can be cumbersome and difficult to read, especially for complicated operations. Enter dplyr. dplyr is a package for making tabular data manipulation easier. It pairs nicely with tidyr which enables you to swiftly convert between different data formats for plotting and analysis.\nThe tidyverse package is an “umbrella-package” that installs tidyr, dplyr, and several other packages useful for data analysis, such as ggplot2, tibble, etc.\nThe tidyverse package tries to address 3 common issues that arise when doing data analysis with some of the functions that come with R:\nThe results from a base R function sometimes depend on the type of data.\nUsing R expressions in a non standard way, which can be confusing for new learners.\nHidden arguments, having default operations that new learners are not aware of.\nYou should already have installed and loaded the tidyverse package. If we haven’t already done so, we can type install.packages(\"tidyverse\") straight into the console. Then, to load the package type library(tidyverse)\nWhat are dplyr and tidyr?\nThe package dplyr provides easy tools for the most common data manipulation tasks. It is built to work directly with data frames, with many common tasks optimized by being written in a compiled language (C++). An additional feature is the ability to work directly with data stored in an external database. The benefits of doing this are that the data can be managed natively in a relational database, queries can be conducted on that database, and only the results of the query are returned.\nThis addresses a common problem with R in that all operations are conducted in-memory and thus the amount of data you can work with is limited by available memory. The database connections essentially remove that limitation in that you can connect to a database of many hundreds of GB, conduct queries on it directly, and pull back into R only what you need for analysis.\nThe package tidyr addresses the common problem of wanting to reshape your data for plotting and use by different R functions. Sometimes we want data sets where we have one row per measurement. Sometimes we want a data frame where each measurement type has its own column, and rows are instead more aggregated groups (e.g., a time period, an experimental unit like a plot or a batch number). Moving back and forth between these formats is non-trivial, and tidyr gives you tools for this and more sophisticated data manipulation.\nTo learn more about dplyr and tidyr after the workshop, you may want to check out this handy data transformation with dplyr cheatsheet and this one about tidyr.\nAs before, we’ll read in our data using the read_csv() function from the tidyverse package readr.\n\n\nsurveys <- read_csv(\"data_raw/portal_data_joined.csv\")\n\n\n\n\n\n## inspect the data\nstr(surveys)\n\n\n\n\n\n## preview the data\nview(surveys)\n\n\n\nNext, we’re going to learn some of the most common dplyr functions:\nselect()\nsubset columns\nfilter()\nsubset rows on conditions\nmutate()\ncreate new columns by using information from other columns\ngroup_by() and summarize()\ncreate summary statistics on grouped data\narrange()\nsort results\ncount()\ncount discrete values\nSelecting columns and filtering rows\nTo select columns of a data frame, use select(). The first argument to this function is the data frame (surveys), and the subsequent arguments are the columns to keep.\n\n\nselect(surveys, plot_id, species_id, weight)\n\n\n\nTo select all columns except certain ones, put a “-” in front of the variable to exclude it.\n\n\nselect(surveys, -record_id, -species_id)\n\n\n\nThis will select all the variables in surveys except record_id and species_id.\nTo choose rows based on a specific criterion, use filter():\n\n\nfilter(surveys, year == 1995)\n\n\n\nThe pipe %>%\nWhat if you want to select and filter at the same time? There are three ways to do this: use intermediate steps, nested functions, or pipes.\nWith intermediate steps, you create a temporary data frame and use that as input to the next function, like this:\n\n\nsurveys2 <- filter(surveys, weight < 5)\nsurveys_sml <- select(surveys2, species_id, sex, weight)\n\n\n\nThis is readable, but can clutter up your workspace with lots of objects that you have to name individually. With multiple steps, that can be hard to keep track of.\nYou can also nest functions (i.e. one function inside of another), like this:\n\n\nsurveys_sml <- select(filter(surveys, weight < 5), species_id, sex, weight)\n\n\n\nThis is handy, but can be difficult to read if too many functions are nested, as R evaluates the expression from the inside out (in this case, filtering, then selecting).\nThe last option, pipes, are a recent addition to R. Pipes let you take the output of one function and send it directly to the next, which is useful when you need to do many things to the same dataset. Pipes in R look like %>% and are made available via the magrittr package, installed automatically with dplyr. If you use RStudio, you can type the pipe with Ctrl + Shift + M if you have a PC or Cmd + Shift + M if you have a Mac.\n\n\nsurveys %>%\n  filter(weight < 5) %>%\n  select(species_id, sex, weight)\n\n\n\nIn the above code, we use the pipe to send the surveys dataset first through filter() to keep rows where weight is less than 5, then through select() to keep only the species_id, sex, and weight columns. Since %>% takes the object on its left and passes it as the first argument to the function on its right, we don’t need to explicitly include the data frame as an argument to the filter() and select() functions any more.\nSome may find it helpful to read the pipe like the word “then”. For instance, in the above example, we took the data frame surveys, then we filtered for rows with weight < 5, then we selected columns species_id, sex, and weight. The dplyr functions by themselves are somewhat simple, but by combining them into linear workflows with the pipe, we can accomplish more complex manipulations of data frames.\nIf we want to create a new object with this smaller version of the data, we can assign it a new name:\n\n\nsurveys_sml <- surveys %>%\n  filter(weight < 5) %>%\n  select(species_id, sex, weight)\n\nsurveys_sml\n\n\n\nNote that the final data frame is the leftmost part of this expression.\n\nChallenge\nUsing pipes, subset the surveys data to include animals collected before 1995 and retain only the columns year, sex, and weight.\n\nAnswer\n\nsurveys %>%\n   filter(year < 1995) %>%\n   select(year, sex, weight)\n\n\n\n\n\n\n\n\nMutate\nFrequently you’ll want to create new columns based on the values in existing columns, for example to do unit conversions, or to find the ratio of values in two columns. For this we’ll use mutate().\nTo create a new column of weight in kg:\n\n\nsurveys %>%\n  mutate(weight_kg = weight / 1000)\n\n\n\nYou can also create a second new column based on the first new column within the same call of mutate():\n\n\nsurveys %>%\n  mutate(weight_kg = weight / 1000,\n         weight_lb = weight_kg * 2.2)\n\n\n\nIf this runs off your screen and you just want to see the first few rows, you can use a pipe to view the head() of the data. (Pipes work with non-dplyr functions, too, as long as the dplyr or magrittr package is loaded).\n\n\nsurveys %>%\n  mutate(weight_kg = weight / 1000) %>%\n  head()\n\n\n\nThe first few rows of the output are full of NAs, so if we wanted to remove those we could insert a filter() in the chain:\n\n\nsurveys %>%\n  filter(!is.na(weight)) %>%\n  mutate(weight_kg = weight / 1000) %>%\n  head()\n\n\n\nis.na() is a function that determines whether something is an NA. The ! symbol negates the result, so we’re asking for every row where weight is not an NA.\n\nChallenge\nCreate a new data frame from the surveys data that meets the following criteria: contains only the species_id column and a new column called hindfoot_cm containing the hindfoot_length values converted to centimeters. In this hindfoot_cm column, there are no NAs and all values are less than 3. Hint: think about how the commands should be ordered to produce this data frame!\n\nAnswer\n\n  surveys_hindfoot_cm <- surveys %>%\n  filter(!is.na(hindfoot_length)) %>%\n  mutate(hindfoot_cm = hindfoot_length / 10) %>%\n  filter(hindfoot_cm < 3) %>%\n  select(species_id, hindfoot_cm)\n\n\n\n\n\n\n\n\nSplit-apply-combine data analysis with group_by() and summarize()\nMany data analysis tasks can be approached using the split-apply-combine paradigm: split the data into groups, apply some analysis to each group, and then combine the results. dplyr makes this very easy through the use of the group_by() function.\ngroup_by() is often used together with summarize(), which collapses each group into a single-row summary of that group. group_by() takes as arguments the column names that contain the categorical variables for which you want to calculate the summary statistics. So to compute the mean weight by sex:\n\n\nsurveys %>%\n  group_by(sex) %>%\n  summarize(mean_weight = mean(weight, na.rm = TRUE))\n\n\n\nYou may also have noticed that the output from these calls doesn’t run off the screen anymore. It’s one of the advantages of tbl_df over data frame.\nYou can also group by multiple columns:\n\n\nsurveys %>%\n  group_by(sex, species_id) %>%\n  summarize(mean_weight = mean(weight, na.rm = TRUE)) %>% \n  tail()\n\n\n\nHere, we used tail() to look at the last six rows of our summary. Before, we had used head() to look at the first six rows. We can see that the sex column contains NA values because some animals had escaped before their sex and body weights could be determined. The resulting mean_weight column does not contain NA but NaN (which refers to “Not a Number”) because mean() was called on a vector of NA values while at the same time setting na.rm = TRUE. To avoid this, we can remove the missing values for weight before we attempt to calculate the summary statistics on weight. Because the missing values are removed first, we can omit na.rm = TRUE when computing the mean:\n\n\nsurveys %>%\n  filter(!is.na(weight)) %>%\n  group_by(sex, species_id) %>%\n  summarize(mean_weight = mean(weight))\n\n\n\nHere, again, the output from these calls doesn’t run off the screen anymore. If you want to display more data, you can use the print() function at the end of your chain with the argument n specifying the number of rows to display:\n\n\nsurveys %>%\n  filter(!is.na(weight)) %>%\n  group_by(sex, species_id) %>%\n  summarize(mean_weight = mean(weight)) %>%\n  print(n = 15)\n\n\n\nOnce the data are grouped, you can also summarize multiple variables at the same time (and not necessarily on the same variable). For instance, we could add a column indicating the minimum weight for each species for each sex:\n\n\nsurveys %>%\n  filter(!is.na(weight)) %>%\n  group_by(sex, species_id) %>%\n  summarize(mean_weight = mean(weight),\n            min_weight = min(weight))\n\n\n\nIt is sometimes useful to rearrange the result of a query to inspect the values. For instance, we can sort on min_weight to put the lighter species first:\n\n\nsurveys %>%\n  filter(!is.na(weight)) %>%\n  group_by(sex, species_id) %>%\n  summarize(mean_weight = mean(weight),\n            min_weight = min(weight)) %>%\n  arrange(min_weight)\n\n\n\nTo sort in descending order, we need to add the desc() function. If we want to sort the results by decreasing order of mean weight:\n\n\nsurveys %>%\n  filter(!is.na(weight)) %>%\n  group_by(sex, species_id) %>%\n  summarize(mean_weight = mean(weight),\n            min_weight = min(weight)) %>%\n  arrange(desc(mean_weight))\n\n\n\nCounting\nWhen working with data, we often want to know the number of observations found for each factor or combination of factors. For this task, dplyr provides count(). For example, if we wanted to count the number of rows of data for each sex, we would do:\n\n\nsurveys %>%\n    count(sex) \n\n\n\nThe count() function is shorthand for something we’ve already seen: grouping by a variable, and summarizing it by counting the number of observations in that group. In other words, surveys %>% count() is equivalent to:\n\n\nsurveys %>%\n    group_by(sex) %>%\n    summarise(count = n())\n\n\n\nFor convenience, count() provides the sort argument:\n\n\nsurveys %>%\n    count(sex, sort = TRUE) \n\n\n\nPrevious example shows the use of count() to count the number of rows/observations for one factor (i.e., sex). If we wanted to count combination of factors, such as sex and species, we would specify the first and the second factor as the arguments of count():\n\n\nsurveys %>%\n  count(sex, species) \n\n\n\nWith the above code, we can proceed with arrange() to sort the table according to a number of criteria so that we have a better comparison. For instance, we might want to arrange the table above in (i) an alphabetical order of the levels of the species and (ii) in descending order of the count:\n\n\nsurveys %>%\n  count(sex, species) %>%\n  arrange(species, desc(n))\n\n\n\nFrom the table above, we may learn that, for instance, there are 75 observations of the albigula species that are not specified for its sex (i.e. NA).\n\nChallenge\nHow many animals were caught in each plot_type surveyed?\n\nAnswer\n\nsurveys %>%\n   count(plot_type) \n\n\n\n\nUse group_by() and summarize() to find the mean, min, and max hindfoot length for each species (using species_id). Also add the number of observations (hint: see ?n).\n\nAnswer\n\nsurveys %>%\n   filter(!is.na(hindfoot_length)) %>%\n   group_by(species_id) %>%\n   summarize(\n       mean_hindfoot_length = mean(hindfoot_length),\n       min_hindfoot_length = min(hindfoot_length),\n       max_hindfoot_length = max(hindfoot_length),\n       n = n()\n   )\n\n\n\n\nWhat was the heaviest animal measured in each year? Return the columns year, genus, species_id, and weight.\n\nAnswer\n\nsurveys %>%\n   filter(!is.na(weight)) %>%\n   group_by(year) %>%\n   filter(weight == max(weight)) %>%\n   select(year, genus, species, weight) %>%\n   arrange(year)\n\n\n\n\n\n\n\n\nReshaping with gather and spread\nIn the spreadsheet lesson, we discussed how to structure our data leading to the four rules defining a tidy dataset:\nEach variable has its own column\nEach observation has its own row\nEach value must have its own cell\nEach type of observational unit forms a table\nHere we examine the fourth rule: Each type of observational unit forms a table.\nIn surveys, the rows of surveys contain the values of variables associated with each record (the unit), values such as the weight or sex of each animal associated with each record. What if instead of comparing records, we wanted to compare the different mean weight of each genus between plots? (Ignoring plot_type for simplicity).\nWe’d need to create a new table where each row (the unit) is comprised of values of variables associated with each plot. In practical terms this means the values in genus would become the names of column variables and the cells would contain the values of the mean weight observed on each plot.\nHaving created a new table, it is therefore straightforward to explore the relationship between the weight of different genera within, and between, the plots. The key point here is that we are still following a tidy data structure, but we have reshaped the data according to the observations of interest: average genus weight per plot instead of recordings per date.\nThe opposite transformation would be to transform column names into values of a variable.\nWe can do both these of transformations with two tidyr functions, spread() and gather().\nSpreading\nspread() takes three principal arguments:\nthe data\nthe key column variable whose values will become new column names.\nthe value column variable whose values will fill the new column variables.\nFurther arguments include fill which, if set, fills in missing values with the value provided.\nLet’s use spread() to transform surveys to find the mean weight of each genus in each plot over the entire survey period. We use filter(), group_by() and summarise() to filter our observations and variables of interest, and create a new variable for the mean_weight.\n\n\nsurveys_gw <- surveys %>%\n  filter(!is.na(weight)) %>%\n  group_by(plot_id, genus) %>%\n  summarize(mean_weight = mean(weight))\n\nstr(surveys_gw)\n\n\n\nThis yields surveys_gw where the observations for each plot are spread across multiple rows, 196 observations of 3 variables. Using spread() to key on genus with values from mean_weight this becomes 24 observations of 11 variables, one row for each plot.\n\n\nsurveys_spread <- surveys_gw %>%\n  spread(key = genus, value = mean_weight)\n\nstr(surveys_spread)\n\n\n\n\nWe could now plot comparisons between the weight of genera (one is called a genus, multiple are called genera) in different plots, although we may wish to fill in the missing values first.\n\n\nsurveys_gw %>%\n  spread(genus, mean_weight, fill = 0) %>%\n  head()\n\n\n\nGathering\nThe opposing situation could occur if we had been provided with data in the form of surveys_spread, where the genus names are column names, but we wish to treat them as values of a genus variable instead.\nIn this situation we are gathering the column names and turning them into a pair of new variables. One variable represents the column names as values, and the other variable contains the values previously associated with the column names.\ngather() takes four principal arguments:\nthe data\nthe key column variable we wish to create from column names.\nthe values column variable we wish to create and fill with values associated with the key.\nthe names of the columns we use to fill the key variable (or to drop).\nTo recreate surveys_gw from surveys_spread we would create a key called genus and value called mean_weight and use all columns except plot_id for the key variable. Here we exclude plot_id from being gather()ed.\n\n\nsurveys_gather <- surveys_spread %>%\n  gather(key = \"genus\", value = \"mean_weight\", -plot_id)\n\nstr(surveys_gather)\n\n\n\n\nNote that now the NA genera are included in the re-gathered format. Spreading and then gathering can be a useful way to balance out a dataset so every replicate has the same composition.\nWe could also have used a specification for what columns to include. This can be useful if you have a large number of identifying columns, and it’s easier to specify what to gather than what to leave alone. And if the columns are directly adjacent, we don’t even need to list them all out - just use the : operator!\n\n\nsurveys_spread %>%\n  gather(key = \"genus\", value = \"mean_weight\", Baiomys:Spermophilus) %>%\n  head()\n\n\n\n\nChallenge\nSpread the surveys data frame with year as columns, plot_id as rows, and the number of genera per plot as the values. You will need to summarize before reshaping, and use the function n_distinct() to get the number of unique genera within a particular chunk of data. It’s a powerful function! See ?n_distinct for more.\n\nAnswer\n\nsurveys_spread_genera <- surveys %>%\n group_by(plot_id, year) %>%\n summarize(n_genera = n_distinct(genus)) %>%\n spread(year, n_genera)\n\nhead(surveys_spread_genera)\n\n\n\n\nNow take that data frame and gather() it again, so each row is a unique plot_id by year combination.\n\nAnswer\n\nsurveys_spread_genera %>%\n gather(\"year\", \"n_genera\", -plot_id)\n\n\n\n\nThe surveys data set has two measurement columns: hindfoot_length and weight. This makes it difficult to do things like look at the relationship between mean values of each measurement per year in different plot types. Let’s walk through a common solution for this type of problem. First, use gather() to create a dataset where we have a key column called measurement and a value column that takes on the value of either hindfoot_length or weight. Hint: You’ll need to specify which columns are being gathered.\n\nAnswer\n\nsurveys_long <- surveys %>%\n gather(\"measurement\", \"value\", hindfoot_length, weight)\n\n\n\n\nWith this new data set, calculate the average of each measurement in each year for each different plot_type. Then spread() them into a data set with a column for hindfoot_length and weight. Hint: You only need to specify the key and value columns for spread().\n\nAnswer\n\nsurveys_long %>%\n group_by(year, measurement, plot_type) %>%\n summarize(mean_value = mean(value, na.rm=TRUE)) %>%\n spread(measurement, mean_value)\n\n\n\n\n\n\n\n\nExporting data\nNow that you have learned how to use dplyr to extract information from or summarize your raw data, you may want to export these new data sets to share them with your collaborators or for archival.\nSimilar to the read_csv() function used for reading CSV files into R, there is a write_csv() function that generates CSV files from data frames.\nBefore using write_csv(), we are going to create a new folder, data, in our working directory that will store this generated dataset. We don’t want to write generated datasets in the same directory as our raw data. It’s good practice to keep them separate. The data_raw folder should only contain the raw, unaltered data, and should be left alone to make sure we don’t delete or modify it. In contrast, our script will generate the contents of the data directory, so even if the files it contains are deleted, we can always re-generate them.\nIn preparation for our next lesson on plotting, we are going to prepare a cleaned up version of the data set that doesn’t include any missing data.\nLet’s start by removing observations of animals for which weight and hindfoot_length are missing, or the sex has not been determined:\n\n\nsurveys_complete <- surveys %>%\n  filter(!is.na(weight),           # remove missing weight\n         !is.na(hindfoot_length),  # remove missing hindfoot_length\n         !is.na(sex))                # remove missing sex\n\n\n\nBecause we are interested in plotting how species abundances have changed through time, we are also going to remove observations for rare species (i.e., that have been observed less than 50 times). We will do this in two steps: first we are going to create a data set that counts how often each species has been observed, and filter out the rare species; then, we will extract only the observations for these more common species:\n\n\n## Extract the most common species_id\nspecies_counts <- surveys_complete %>%\n    count(species_id) %>% \n    filter(n >= 50)\n\n## Only keep the most common species\nsurveys_complete <- surveys_complete %>%\n  filter(species_id %in% species_counts$species_id)\n\n\n\n\n\n\nTo make sure that everyone has the same data set, check that surveys_complete has 30463 rows and 13 columns by typing dim(surveys_complete).\nNow that our data set is ready, we can save it as a CSV file in our data folder.\n\n\nwrite_csv(surveys_complete, file = \"data/surveys_complete.csv\")\n\n\n\n\n\n\n\n\n\n",
      "last_modified": "2020-10-29T19:13:58+00:00"
    },
    {
      "path": "04-visualization-ggplot2.html",
      "title": "Data visualization with ggplot2",
      "author": [],
      "contents": "\n\nContents\nPlotting with ggplot2\nBuilding your plots iteratively\nBoxplot\nPlotting time series data\nIntegrating the pipe operator with ggplot2\nFaceting\nggplot2 themes\nCustomization\nArranging plots\nExporting plots\n\n\n\n\n\n\n\n\nLearning Objectives\nProduce scatter plots, boxplots, and time series plots using ggplot.\nSet universal plot settings.\nDescribe what faceting is and apply faceting in ggplot.\nModify the aesthetics of an existing ggplot plot (including axis labels and color).\nBuild complex and customized plots from data in a data frame.\n\nWe start by loading the required packages. ggplot2 is included in the tidyverse package.\n\n\nlibrary(tidyverse)\n\n\n\nIf not still in the workspace, load the data we saved in the previous lesson.\n\n\nsurveys_complete <- read_csv(\"data/surveys_complete.csv\")\n\n\n\nPlotting with ggplot2\nggplot2 is a plotting package that makes it simple to create complex plots from data in a data frame. It provides a more programmatic interface for specifying what variables to plot, how they are displayed, and general visual properties. Therefore, we only need minimal changes if the underlying data change or if we decide to change from a bar plot to a scatterplot. This helps in creating publication quality plots with minimal amounts of adjustments and tweaking.\nggplot2 functions like data in the ‘long’ format, i.e., a column for every dimension, and a row for every observation. Well-structured data will save you lots of time when making figures with ggplot2\nggplot graphics are built step by step by adding new elements. Adding layers in this fashion allows for extensive flexibility and customization of plots.\nTo build a ggplot, we will use the following basic template that can be used for different types of plots:\nggplot(data = <DATA>, mapping = aes(<MAPPINGS>)) +  <GEOM_FUNCTION>()\nuse the ggplot() function and bind the plot to a specific data frame using the data argument\n\n\nggplot(data = surveys_complete)\n\n\n\ndefine an aesthetic mapping (using the aesthetic (aes) function), by selecting the variables to be plotted and specifying how to present them in the graph, e.g. as x/y positions or characteristics such as size, shape, color, etc.\n\n\nggplot(data = surveys_complete, mapping = aes(x = weight, y = hindfoot_length))\n\n\n\nadd ‘geoms’ – graphical representations of the data in the plot (points, lines, bars). ggplot2 offers many different geoms; we will use some common ones today, including:\ngeom_point() for scatter plots, dot plots, etc.\ngeom_boxplot() for, well, boxplots!\ngeom_line() for trend lines, time series, etc.\n\nTo add a geom to the plot use + operator. Because we have two continuous variables, let’s use geom_point() first:\n\n\nggplot(data = surveys_complete, aes(x = weight, y = hindfoot_length)) +\n  geom_point()\n\n\n\n\nThe + in the ggplot2 package is particularly useful because it allows you to modify existing ggplot objects. This means you can easily set up plot “templates” and conveniently explore different types of plots, so the above plot can also be generated with code like this:\n\n\n# Assign plot to a variable\nsurveys_plot <- ggplot(data = surveys_complete, \n                       mapping = aes(x = weight, y = hindfoot_length))\n\n# Draw the plot\nsurveys_plot + \n    geom_point()\n\n\n\n\n\n\nNotes\nAnything you put in the ggplot() function can be seen by any geom layers that you add (i.e., these are universal plot settings). This includes the x- and y-axis you set up in aes().\nYou can also specify aesthetics for a given geom independently of the aesthetics defined globally in the ggplot() function.\nThe + sign used to add layers must be placed at the end of each line containing a layer. If, instead, the + sign is added in the line before the other layer, ggplot2 will not add the new layer and will return an error message.\n\n\n# This is the correct syntax for adding layers\nsurveys_plot +\n  geom_point()\n\n# This will not add the new layer and will return an error message\nsurveys_plot\n  + geom_point()\n\n\n\n\nChallenge (optional)\nScatter plots can be useful exploratory tools for small datasets. For data sets with large numbers of observations, such as the surveys_complete data set, overplotting of points can be a limitation of scatter plots. One strategy for handling such settings is to use hexagonal binning of observations. The plot space is tessellated into hexagons. Each hexagon is assigned a color based on the number of observations that fall within its boundaries. To use hexagonal binning with ggplot2, first install the R package hexbin from CRAN:\n\n\ninstall.packages(\"hexbin\")\nlibrary(hexbin)\n\n\n\nThen use the geom_hex() function:\n\n\nsurveys_plot +\n  geom_hex()\n\n\n\nWhat are the relative strengths and weaknesses of a hexagonal bin plot compared to a scatter plot? Examine the above scatter plot and compare it with the hexagonal bin plot that you created.\n\n\n\n\nBuilding your plots iteratively\nBuilding plots with ggplot2 is typically an iterative process. We start by defining the dataset we’ll use, lay out the axes, and choose a geom:\n\n\nggplot(data = surveys_complete, aes(x = weight, y = hindfoot_length)) +\n    geom_point()\n\n\n\n\nThen, we start modifying this plot to extract more information from it. For instance, we can add transparency (alpha) to avoid overplotting:\n\n\nggplot(data = surveys_complete, aes(x = weight, y = hindfoot_length)) +\n    geom_point(alpha = 0.1)\n\n\n\n\nWe can also add colors for all the points:\n\n\nggplot(data = surveys_complete, mapping = aes(x = weight, y = hindfoot_length)) +\n    geom_point(alpha = 0.1, color = \"blue\")\n\n\n\n\nOr to color each species in the plot differently, you could use a vector as an input to the argument color. ggplot2 will provide a different color corresponding to different values in the vector. Here is an example where we color with species_id:\n\n\nggplot(data = surveys_complete, mapping = aes(x = weight, y = hindfoot_length)) +\n    geom_point(alpha = 0.1, aes(color = species_id))\n\n\n\n\n\nChallenge\nUse what you just learned to create a scatter plot of weight over species_id with the plot types showing in different colors. Is this a good way to show this type of data?\n\nAnswer\n\nggplot(data = surveys_complete, \n       mapping = aes(x = species_id, y = weight)) +\n  geom_point(aes(color = plot_type))\n\n\n\n\n\n\n\nBoxplot\nWe can use boxplots to visualize the distribution of weight within each species:\n\n\nggplot(data = surveys_complete, mapping = aes(x = species_id, y = weight)) +\n    geom_boxplot()\n\n\n\n\nBy adding points to the boxplot, we can have a better idea of the number of measurements and of their distribution:\n\n\nggplot(data = surveys_complete, mapping = aes(x = species_id, y = weight)) +\n    geom_boxplot(alpha = 0) +\n    geom_jitter(alpha = 0.3, color = \"tomato\")\n\n\n\n\nNotice how the boxplot layer is behind the jitter layer? What do you need to change in the code to put the boxplot in front of the points such that it’s not hidden?\n\nChallenges\nBoxplots are useful summaries, but hide the shape of the distribution. For example, if there is a bimodal distribution, it would not be observed with a boxplot. An alternative to the boxplot is the violin plot (sometimes known as a beanplot), where the shape (of the density of points) is drawn.\nReplace the box plot with a violin plot; see geom_violin().\nIn many types of data, it is important to consider the scale of the observations. For example, it may be worth changing the scale of the axis to better distribute the observations in the space of the plot. Changing the scale of the axes is done similarly to adding/modifying other components (i.e., by incrementally adding commands). Try making these modifications:\nRepresent weight on the log10 scale; see scale_y_log10().\nSo far, we’ve looked at the distribution of weight within species. Try making a new plot to explore the distribution of another variable within each species.\nCreate boxplot for hindfoot_length. Overlay the boxplot layer on a jitter layer to show actual measurements.\nAdd color to the data points on your boxplot according to the plot from which the sample was taken (plot_id).\nHint: Check the class for plot_id. Consider changing the class of plot_id from integer to factor. Why does this change how R makes the graph?\n\n\n\n\nPlotting time series data\nLet’s calculate number of counts per year for each genus. First we need to group the data and count records within each group:\n\n\nyearly_counts <- surveys_complete %>%\n  count(year, genus)\n\n\n\nTimelapse data can be visualized as a line plot with years on the x-axis and counts on the y-axis:\n\n\nggplot(data = yearly_counts, aes(x = year, y = n)) +\n     geom_line()\n\n\n\n\nUnfortunately, this does not work because we plotted data for all the genera together. We need to tell ggplot to draw a line for each genus by modifying the aesthetic function to include group = genus:\n\n\nggplot(data = yearly_counts, aes(x = year, y = n, group = genus)) +\n    geom_line()\n\n\n\n\nWe will be able to distinguish species in the plot if we add colors (using color also automatically groups the data):\n\n\nggplot(data = yearly_counts, aes(x = year, y = n, color = genus)) +\n    geom_line()\n\n\n\n\nIntegrating the pipe operator with ggplot2\nIn the previous lesson, we saw how to use the pipe operator %>% to use different functions in a sequence and create a coherent workflow. We can also use the pipe operator to pass the data argument to the ggplot() function. The hard part is to remember that to build your ggplot, you need to use + and not %>%.\n\n\nyearly_counts %>% \n    ggplot(mapping = aes(x = year, y = n, color = genus)) +\n    geom_line()\n\n\n\n\nThe pipe operator can also be used to link data manipulation with consequent data visualization.\n\n\nyearly_counts_graph <- surveys_complete %>%\n    count(year, genus) %>% \n    ggplot(mapping = aes(x = year, y = n, color = genus)) +\n    geom_line()\n\nyearly_counts_graph\n\n\n\n\nFaceting\nggplot has a special technique called faceting that allows the user to split one plot into multiple plots based on a factor included in the dataset. We will use it to make a time series plot for each species:\n\n\nggplot(data = yearly_counts, aes(x = year, y = n)) +\n    geom_line() +\n    facet_wrap(facets = vars(genus))\n\n\n\n\nNow we would like to split the line in each plot by the sex of each individual measured. To do that we need to make counts in the data frame grouped by year, genus, and sex:\n\n\n yearly_sex_counts <- surveys_complete %>%\n                      count(year, genus, sex)\n\n\n\nWe can now make the faceted plot by splitting further by sex using color (within a single plot):\n\n\nggplot(data = yearly_sex_counts, mapping = aes(x = year, y = n, color = sex)) +\n  geom_line() +\n  facet_wrap(facets =  vars(genus))\n\n\n\n\nWe can also facet both by sex and genus:\n\n\nggplot(data = yearly_sex_counts, \n       mapping = aes(x = year, y = n, color = sex)) +\n  geom_line() +\n  facet_grid(rows = vars(sex), cols =  vars(genus))\n\n\n\n\nYou can also organise the panels only by rows (or only by columns):\n\n\n# One column, facet by rows\nggplot(data = yearly_sex_counts, \n       mapping = aes(x = year, y = n, color = sex)) +\n  geom_line() +\n  facet_grid(rows = vars(genus))\n\n\n\n\n\n\n# One row, facet by column\nggplot(data = yearly_sex_counts, \n       mapping = aes(x = year, y = n, color = sex)) +\n  geom_line() +\n  facet_grid(cols = vars(genus))\n\n\n\n\nNote: ggplot2 before version 3.0.0 used formulas to specify how plots are faceted. If you encounter facet_grid/wrap(...) code containing ~, please read https://ggplot2.tidyverse.org/news/#tidy-evaluation.\nggplot2 themes\nUsually plots with white background look more readable when printed. Every single component of a ggplot graph can be customized using the generic theme() function, as we will see below. However, there are pre-loaded themes available that change the overall appearance of the graph without much effort.\nFor example, we can change our previous graph to have a simpler white background using the theme_bw() function:\n\n\n ggplot(data = yearly_sex_counts, \n        mapping = aes(x = year, y = n, color = sex)) +\n     geom_line() +\n     facet_wrap(vars(genus)) +\n     theme_bw()\n\n\n\n\nIn addition to theme_bw(), which changes the plot background to white, ggplot2 comes with several other themes which can be useful to quickly change the look of your visualization. The complete list of themes is available at https://ggplot2.tidyverse.org/reference/ggtheme.html. theme_minimal() and theme_light() are popular, and theme_void() can be useful as a starting point to create a new hand-crafted theme.\nThe ggthemes package provides a wide variety of options.\n\nChallenge\nUse what you just learned to create a plot that depicts how the average weight of each species changes through the years.\n\nAnswer\n\nyearly_weight <- surveys_complete %>%\n                 group_by(year, species_id) %>%\n                  summarize(avg_weight = mean(weight))\nggplot(data = yearly_weight, mapping = aes(x=year, y=avg_weight)) +\n  geom_line() +\n  facet_wrap(vars(species_id)) +\n  theme_bw()\n\n\n\n\n\n\n\nCustomization\nTake a look at the ggplot2 cheat sheet, and think of ways you could improve the plot.\nNow, let’s change names of axes to something more informative than ‘year’ and ‘n’ and add a title to the figure:\n\n\nggplot(data = yearly_sex_counts, aes(x = year, y = n, color = sex)) +\n    geom_line() +\n    facet_wrap(vars(genus)) +\n    labs(title = \"Observed genera through time\",\n         x = \"Year of observation\",\n         y = \"Number of individuals\") +\n    theme_bw()\n\n\n\n\nThe axes have more informative names, but their readability can be improved by increasing the font size. This can be done with the generic theme() function:\n\n\nggplot(data = yearly_sex_counts, mapping = aes(x = year, y = n, color = sex)) +\n    geom_line() +\n    facet_wrap(vars(genus)) +\n    labs(title = \"Observed genera through time\",\n        x = \"Year of observation\",\n        y = \"Number of individuals\") +\n    theme_bw() +\n    theme(text=element_text(size = 16))\n\n\n\n\nNote that it is also possible to change the fonts of your plots. If you are on Windows, you may have to install the extrafont package, and follow the instructions included in the README for this package.\nAfter our manipulations, you may notice that the values on the x-axis are still not properly readable. Let’s change the orientation of the labels and adjust them vertically and horizontally so they don’t overlap. You can use a 90 degree angle, or experiment to find the appropriate angle for diagonally oriented labels. We can also modify the facet label text (strip.text) to italicize the genus names:\n\n\nggplot(data = yearly_sex_counts, mapping = aes(x = year, y = n, color = sex)) +\n    geom_line() +\n    facet_wrap(vars(genus)) +\n    labs(title = \"Observed genera through time\",\n        x = \"Year of observation\",\n        y = \"Number of individuals\") +\n    theme_bw() +\n    theme(axis.text.x = element_text(colour = \"grey20\", size = 12, angle = 90, hjust = 0.5, vjust = 0.5),\n                        axis.text.y = element_text(colour = \"grey20\", size = 12),\n                        strip.text = element_text(face = \"italic\"),\n                        text = element_text(size = 16))\n\n\n\n\nIf you like the changes you created better than the default theme, you can save them as an object to be able to easily apply them to other plots you may create:\n\n\ngrey_theme <- theme(axis.text.x = element_text(colour=\"grey20\", size = 12, \n                                               angle = 90, hjust = 0.5, \n                                               vjust = 0.5),\n                    axis.text.y = element_text(colour = \"grey20\", size = 12),\n                    text=element_text(size = 16))\n\nggplot(surveys_complete, aes(x = species_id, y = hindfoot_length)) +\n    geom_boxplot() +\n    grey_theme\n\n\n\n\n\nChallenge\nWith all of this information in hand, please take another five minutes to either improve one of the plots generated in this exercise or create a beautiful graph of your own. Use the RStudio ggplot2 cheat sheet for inspiration.\nHere are some ideas:\nSee if you can change the thickness of the lines.\nCan you find a way to change the name of the legend? What about its labels?\nTry using a different color palette (see http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/).\n\n\n\n\nArranging plots\nFaceting is a great tool for splitting one plot into multiple plots, but sometimes you may want to produce a single figure that contains multiple plots using different variables or even different data frames. The patchwork package allows us to combine separate ggplots into a single figure while keeping everything aligned properly. Like most R packages, we can install patchwork from CRAN, the R package repository:\n\n\ninstall.packages(\"patchwork\")\n\n\n\nAfter you have loaded the patchwork package you can use + to place plots next to each other, / to arrange them vertically, and plot_layout() to determine how much space each plot uses:\n\n\nlibrary(patchwork)\n\nplot1 <- ggplot(data = surveys_complete, aes(x = species_id, y = weight)) +\n  geom_boxplot() +\n  labs(x = \"Species\", y = expression(log[10](Weight))) +\n  scale_y_log10()\n\nplot2 <- ggplot(data = yearly_counts, aes(x = year, y = n, color = genus)) +\n  geom_line() + \n  labs(x = \"Year\", y = \"Abundance\")\n\nplot1 / plot2 + plot_layout(heights = c(3, 2))\n\n\n\n\nYou can also use parentheses () to create more complex layouts. There are many useful examples on the patchwork website\nExporting plots\nAfter creating your plot, you can save it to a file in your favorite format. The Export tab in the Plot pane in RStudio will save your plots at low resolution, which will not be accepted by many journals and will not scale well for posters. The ggplot2 extensions website provides a list of packages that extend the capabilities of ggplot2, including additional themes.\nInstead, use the ggsave() function, which allows you easily change the dimension and resolution of your plot by adjusting the appropriate arguments (width, height and dpi):\n\n\nmy_plot <- ggplot(data = yearly_sex_counts, \n                  aes(x = year, y = n, color = sex)) +\n    geom_line() +\n    facet_wrap(vars(genus)) +\n    labs(title = \"Observed genera through time\",\n        x = \"Year of observation\",\n        y = \"Number of individuals\") +\n    theme_bw() +\n    theme(axis.text.x = element_text(colour = \"grey20\", size = 12, angle = 90,\n                                     hjust = 0.5, vjust = 0.5),\n          axis.text.y = element_text(colour = \"grey20\", size = 12),\n          text = element_text(size = 16))\n\nggsave(\"name_of_file.png\", my_plot, width = 15, height = 10)\n\n## This also works for grid.arrange() plots\ncombo_plot <- grid.arrange(spp_weight_boxplot, spp_count_plot, ncol = 2, \n                           widths = c(4, 6))\nggsave(\"combo_plot_abun_weight.png\", combo_plot, width = 10, dpi = 300)\n\n\n\nNote: The parameters width and height also determine the font size in the saved plot.\n\n\n\n",
      "last_modified": "2020-10-29T19:14:29+00:00"
    },
    {
      "path": "05-r-and-databases.html",
      "title": "SQL databases and R",
      "author": [],
      "contents": "\n\nContents\nIntroductionThe portal_mammals database\n\nConnecting to databases\nQuerying the databaseQuerying the database with SQL syntax\nQuerying the database with the dplyr syntax\nSQL translation\n\nSimple database queriesLaziness\n\nComplex database queries\nCreating a new SQLite database\n\n\n\n\n\nLearning Objectives\nAccess a database from R.\nRun SQL queries in R using RSQLite and dplyr.\nDescribe the lazy behavior of dplyr on data stored in a database outside of R.\nPrototype queries and retrieve all final results.\nCreate complex queries across one or multiple database tables.\nCreate an SQLite database from existing .csv files.\n\nIntroduction\nSo far, we have dealt with small datasets that easily fit into your computer’s memory. But what about datasets that are too large for your computer to handle as a whole? In this case, storing the data outside of R and organizing it in a database is helpful. Connecting to the database allows you to retrieve only the chunks needed for the current analysis.\nEven better, many large datasets are already available in public or private databases. You can query them without having to download the data first.\nR can connect to almost any existing database type. Most common database types have R packages that allow you to connect to them (e.g., RSQLite, RMySQL, etc). Furthermore, the dplyr package you used in the previous chapter, in conjunction with dbplyr supports connecting to the widely-used open source databases sqlite, mysql and postgresql, as well as Google’s bigquery, and it can also be extended to other database types (a vignette in the dplyr package explains how to do it). RStudio has created a website that provides documentation and best practices to work on database interfaces.\nInterfacing with databases using dplyr focuses on retrieving and analyzing datasets by generating SELECT SQL statements, but it doesn’t modify the database itself. dplyr does not offer functions to UPDATE or DELETE entries. If you need these functionalities, you will need to use additional R packages (e.g., RSQLite). Here we will demonstrate how to interact with a database using dplyr, using both the dplyr’s verb syntax and the SQL syntax.\nThe portal_mammals database\nWe will continue to explore the surveys data you are already familiar with from previous lessons. First, we are going to install the dbplyr package:\n\n\ninstall.packages(c(\"dbplyr\", \"RSQLite\"))\n\n\n\nThe SQLite database is contained in a single file portal_mammals.sqlite that you generated during the SQL lesson. If you don’t have it, you can download it from Figshare into the data_raw subdirectory using:\n\n\ndir.create(\"data_raw\", showWarnings = FALSE)\ndownload.file(url = \"https://ndownloader.figshare.com/files/2292171\",\n              destfile = \"data_raw/portal_mammals.sqlite\", mode = \"wb\")\n\n\n\nConnecting to databases\nWe can point R to this database using:\n\n\nlibrary(dplyr)\nlibrary(dbplyr)\nmammals <- DBI::dbConnect(RSQLite::SQLite(), \"data_raw/portal_mammals.sqlite\")\n\n\n\nThis command uses 2 packages that helps dbplyr and dplyr talk to the SQLite database. DBI is not something that you’ll use directly as a user. It allows R to send commands to databases irrespective of the database management system used. The RSQLite package allows R to interface with SQLite databases.\nThis command does not load the data into the R session (as the read_csv() function did). Instead, it merely instructs R to connect to the SQLite database contained in the portal_mammals.sqlite file.\nUsing a similar approach, you could connect to many other database management systems that are supported by R including MySQL, PostgreSQL, BigQuery, etc.\nLet’s take a closer look at the mammals database we just connected to:\n\n\nsrc_dbi(mammals)\n\n\n#> src:  sqlite 3.33.0 [/home/runner/work/R-ecology-lesson/R-ecology-lesson/data_raw/portal_mammals.sqlite]\n#> tbls: plots, species, surveys\n\nJust like a spreadsheet with multiple worksheets, a SQLite database can contain multiple tables. In this case three of them are listed in the tbls row in the output above:\nplots\nspecies\nsurveys\nNow that we know we can connect to the database, let’s explore how to get the data from its tables into R.\nQuerying the database\nQuerying the database with SQL syntax\nTo connect to tables within a database, you can use the tbl() function from dplyr. This function can be used to send SQL queries to the database. To demonstrate this functionality, let’s select the columns “year”, “species_id”, and “plot_id” from the surveys table:\n\n\ntbl(mammals, sql(\"SELECT year, species_id, plot_id FROM surveys\"))\n\n\n\nWith this approach you can use any of the SQL queries we have seen in the database lesson.\nQuerying the database with the dplyr syntax\nOne of the strengths of dplyr is that the same operation can be done using dplyr’s verbs instead of writing SQL. First, we select the table on which to do the operations by creating the surveys object, and then we use the standard dplyr syntax as if it were a data frame:\n\n\nsurveys <- tbl(mammals, \"surveys\")\nsurveys %>%\n    select(year, species_id, plot_id)\n\n\n\nIn this case, the surveys object behaves like a data frame. Several functions that can be used with data frames can also be used on tables from a database. For instance, the head() function can be used to check the first 10 rows of the table:\n\n\nhead(surveys, n = 10)\n\n\n#> # Source:   lazy query [?? x 9]\n#> # Database: sqlite 3.33.0\n#> #   [/home/runner/work/R-ecology-lesson/R-ecology-lesson/data_raw/portal_mammals.sqlite]\n#>    record_id month   day  year plot_id species_id sex  \n#>        <int> <int> <int> <int>   <int> <chr>      <chr>\n#>  1         1     7    16  1977       2 NL         M    \n#>  2         2     7    16  1977       3 NL         M    \n#>  3         3     7    16  1977       2 DM         F    \n#>  4         4     7    16  1977       7 DM         M    \n#>  5         5     7    16  1977       3 DM         M    \n#>  6         6     7    16  1977       1 PF         M    \n#>  7         7     7    16  1977       2 PE         F    \n#>  8         8     7    16  1977       1 DM         M    \n#>  9         9     7    16  1977       1 DM         F    \n#> 10        10     7    16  1977       6 PF         F    \n#> # … with 2 more variables: hindfoot_length <int>, weight <int>\n\nThis output of the head command looks just like a regular data.frame: The table has 9 columns and the head() command shows us the first 10 rows. Note that the columns plot_type, taxa, genus, and species are missing. These are now located in the tables plots and species which we will join together in a moment.\nHowever, some functions don’t work quite as expected. For instance, let’s check how many rows there are in total using nrow():\n\n\nnrow(surveys)\n\n\n#> [1] NA\n\nThat’s strange - R doesn’t know how many rows the surveys table contains - it returns NA instead. You might have already noticed that the first line of the head() output included ?? indicating that the number of rows wasn’t known.\nThe reason for this behavior highlights a key difference between using dplyr on datasets in memory (e.g. loaded into your R session via read_csv()) and those provided by a database. To understand it, we take a closer look at how dplyr communicates with our SQLite database.\nSQL translation\nRelational databases typically use a special-purpose language, Structured Query Language (SQL), to manage and query data.\nFor example, the following SQL query returns the first 10 rows from the surveys table:\nSELECT *\nFROM `surveys`\nLIMIT 10\nBehind the scenes, dplyr:\ntranslates your R code into SQL\nsubmits it to the database\ntranslates the database’s response into an R data frame\nTo lift the curtain, we can use dplyr’s show_query() function to show which SQL commands are actually sent to the database:\n\n\nshow_query(head(surveys, n = 10))\n\n\n\nThe output shows the actual SQL query sent to the database; it matches our manually constructed SELECT statement above.\nInstead of having to formulate the SQL query ourselves - and having to mentally switch back and forth between R and SQL syntax - we can delegate this translation to dplyr. (You don’t even need to know SQL to interact with a database via dplyr!)\ndplyr, in turn, doesn’t do the real work of subsetting the table, either. Instead, it merely sends the query to the database, waits for its response and returns it to us.\nThat way, R never gets to see the full surveys table - and that’s why it could not tell us how many rows it contains. On the bright side, this allows us to work with large datasets - even too large to fit into our computer’s memory.\ndplyr can translate many different query types into SQL allowing us to, e.g., select() specific columns, filter() rows, or join tables.\nTo see this in action, let’s compose a few queries with dplyr.\nSimple database queries\nFirst, let’s only request rows of the surveys table in which weight is less than 5 and keep only the species_id, sex, and weight columns.\n\n\nsurveys %>%\n  filter(weight < 5) %>%\n  select(species_id, sex, weight)\n\n\n#> # Source:   lazy query [?? x 3]\n#> # Database: sqlite 3.33.0\n#> #   [/home/runner/work/R-ecology-lesson/R-ecology-lesson/data_raw/portal_mammals.sqlite]\n#>    species_id sex   weight\n#>    <chr>      <chr>  <int>\n#>  1 PF         M          4\n#>  2 PF         F          4\n#>  3 PF         <NA>       4\n#>  4 PF         F          4\n#>  5 PF         F          4\n#>  6 RM         M          4\n#>  7 RM         F          4\n#>  8 RM         M          4\n#>  9 RM         M          4\n#> 10 RM         M          4\n#> # … with more rows\n\nExecuting this command will return a table with 10 rows and the requested species_id, sex and weight columns. Great!\n… but wait, why are there only 10 rows?\nThe last line:\n# ... with more rows\nindicates that there are more results that fit our filtering criterion. Why was R lazy and only retrieved 10 of them?\nLaziness\nHadley Wickham, the author of dplyr explains:\n\nWhen working with databases, dplyr tries to be as lazy as possible:\nIt never pulls data into R unless you explicitly ask for it.\nIt delays doing any work until the last possible moment - it collects together everything you want to do and then sends it to the database in one step.\n\nWhen you construct a dplyr query, you can connect multiple verbs into a single pipeline. For example, we combined the filter() and select() verbs using the %>% pipe.\nIf we wanted to, we could add on even more steps, e.g. remove the sex column in an additional select call:\n\n\ndata_subset <- surveys %>%\n  filter(weight < 5) %>%\n  select(species_id, sex, weight)\n\ndata_subset %>%\n  select(-sex)\n\n\n#> # Source:   lazy query [?? x 2]\n#> # Database: sqlite 3.33.0\n#> #   [/home/runner/work/R-ecology-lesson/R-ecology-lesson/data_raw/portal_mammals.sqlite]\n#>    species_id weight\n#>    <chr>       <int>\n#>  1 PF              4\n#>  2 PF              4\n#>  3 PF              4\n#>  4 PF              4\n#>  5 PF              4\n#>  6 RM              4\n#>  7 RM              4\n#>  8 RM              4\n#>  9 RM              4\n#> 10 RM              4\n#> # … with more rows\n\nJust like the first select(species_id, sex, weight) call, the select(-sex) command is not executed by R. It is sent to the database instead. Only the final result is retrieved and displayed to you.\nOf course, we could always add on more steps, e.g., we could filter by species_id or minimum weight. That’s why R doesn’t retrieve the full set of results - instead it only retrieves the first 10 results from the database by default. (After all, you might want to add an additional step and get the database to do more work…)\nTo instruct R to stop being lazy, e.g. to retrieve all of the query results from the database, we add the collect() command to our pipe. It indicates that our database query is finished: time to get the final results and load them into the R session.\n\n\ndata_subset <- surveys %>%\n  filter(weight < 5) %>%\n  select(species_id, sex, weight) %>%\n  collect()\n\n\n\nNow we have all 17 rows that match our query in a data.frame and can continue to work with them exclusively in R, without communicating with the database.\nComplex database queries\ndplyr enables database queries across one or multiple database tables, using the same single- and multiple-table verbs you encountered previously. This means you can use the same commands regardless of whether you interact with a remote database or local dataset! This is a really useful feature if you work with large datasets: you can first prototype your code on a small subset that fits into memory, and when your code is ready, you can change the input dataset to your full database without having to change the syntax.\nOn the other hand, being able to use SQL queries directly can be useful if your collaborators have already put together complex queries to prepare the dataset that you need for your analysis.\nTo illustrate how to use dplyr with these complex queries, we are going to join the plots and surveys tables. The plots table in the database contains information about the different plots surveyed by the researchers. To access it, we point the tbl() command to it:\n\n\nplots <- tbl(mammals, \"plots\")\nplots\n\n\n#> # Source:   table<plots> [?? x 2]\n#> # Database: sqlite 3.33.0\n#> #   [/home/runner/work/R-ecology-lesson/R-ecology-lesson/data_raw/portal_mammals.sqlite]\n#>    plot_id plot_type                \n#>      <int> <chr>                    \n#>  1       1 Spectab exclosure        \n#>  2       2 Control                  \n#>  3       3 Long-term Krat Exclosure \n#>  4       4 Control                  \n#>  5       5 Rodent Exclosure         \n#>  6       6 Short-term Krat Exclosure\n#>  7       7 Rodent Exclosure         \n#>  8       8 Control                  \n#>  9       9 Spectab exclosure        \n#> 10      10 Rodent Exclosure         \n#> # … with more rows\n\nThe plot_id column also features in the surveys table:\n\n\nsurveys\n\n\n#> # Source:   table<surveys> [?? x 9]\n#> # Database: sqlite 3.33.0\n#> #   [/home/runner/work/R-ecology-lesson/R-ecology-lesson/data_raw/portal_mammals.sqlite]\n#>    record_id month   day  year plot_id species_id sex  \n#>        <int> <int> <int> <int>   <int> <chr>      <chr>\n#>  1         1     7    16  1977       2 NL         M    \n#>  2         2     7    16  1977       3 NL         M    \n#>  3         3     7    16  1977       2 DM         F    \n#>  4         4     7    16  1977       7 DM         M    \n#>  5         5     7    16  1977       3 DM         M    \n#>  6         6     7    16  1977       1 PF         M    \n#>  7         7     7    16  1977       2 PE         F    \n#>  8         8     7    16  1977       1 DM         M    \n#>  9         9     7    16  1977       1 DM         F    \n#> 10        10     7    16  1977       6 PF         F    \n#> # … with more rows, and 2 more variables: hindfoot_length <int>,\n#> #   weight <int>\n\nBecause plot_id is listed in both tables, we can use it to look up matching records, and join the two tables.\nIf we have two tables named x and y with a common column called “ID”, we can join them using ‘join’ functions, two of which are described and illustrated below.\ninner_join() : This returns all rows from x where there are matching values in y, and all columns from x and y.\nleft_join() : This return all rows from x, and all columns from x and y. Rows in x with no match in y will have NA values in the new columns.\nIn both forms of join, if there are multiple matches between x and y, all combinations of the matches are returned. For the full list of ‘join’ functions, check out the tidyverse join page.\nIn our example, the two tables we want to join are ‘plots’ and ‘surveys’.\ndiagram illustrating inner and left joinsFor example, to extract all surveys for the first plot, which has plot_id 1, we can do:\n\n\nplots %>%\n  filter(plot_id == 1) %>%\n  inner_join(surveys) %>%\n  collect()\n\n\n#> # A tibble: 1,995 x 10\n#>    plot_id plot_type record_id month   day  year species_id sex  \n#>      <int> <chr>         <int> <int> <int> <int> <chr>      <chr>\n#>  1       1 Spectab …         6     7    16  1977 PF         M    \n#>  2       1 Spectab …         8     7    16  1977 DM         M    \n#>  3       1 Spectab …         9     7    16  1977 DM         F    \n#>  4       1 Spectab …        78     8    19  1977 PF         M    \n#>  5       1 Spectab …        80     8    19  1977 DS         M    \n#>  6       1 Spectab …       218     9    13  1977 PF         M    \n#>  7       1 Spectab …       222     9    13  1977 DS         M    \n#>  8       1 Spectab …       239     9    13  1977 DS         M    \n#>  9       1 Spectab …       263    10    16  1977 DM         M    \n#> 10       1 Spectab …       270    10    16  1977 DM         F    \n#> # … with 1,985 more rows, and 2 more variables:\n#> #   hindfoot_length <int>, weight <int>\n\nImportant Note: Without the collect() statement, only the first 10 matching rows are returned. By adding collect(), the full set of 1,985 is retrieved.\n\nChallenge\nWrite a query that returns the number of rodents observed in each plot in each year. Hint: Connect to the species table and write a query that joins the species and survey tables together to exclude all non-rodents. The query should return counts of rodents by year. Optional: Write a query in SQL that will produce the same result. You can join multiple tables together using the following syntax where foreign key refers to your unique id (e.g., species_id):\nSELECT table.col, table.col\nFROM table1 JOIN table2\nON table1.key = table2.key\nJOIN table3 ON table2.key = table3.key\n\nAnswer\n\n## with dplyr syntax\nspecies <- tbl(mammals, \"species\")\n\nleft_join(surveys, species) %>%\n filter(taxa == \"Rodent\") %>%\n group_by(taxa, year) %>%\n tally %>%\n collect()\n\n## with SQL syntax\nquery <- paste(\"\nSELECT a.year, b.taxa,count(*) as count\nFROM surveys a\nJOIN species b\nON a.species_id = b.species_id\nAND b.taxa = 'Rodent'\nGROUP BY a.year, b.taxa\",\nsep = \"\" )\n\ntbl(mammals, sql(query))\n\n\n\n\n\n\n\n\n\nChallenge\nWrite a query that returns the total number of rodents in each genus caught in the different plot types. Hint: Write a query that joins the species, plot, and survey tables together. The query should return counts of genus by plot type.\n\nAnswer\n\nspecies <- tbl(mammals, \"species\")\ngenus_counts <- left_join(surveys, plots) %>%\n left_join(species) %>%\n filter(taxa == \"Rodent\") %>%\n group_by(plot_type, genus) %>%\n tally %>%\n collect()\n\n\n\n\n\n\n\n\nThis is useful if we are interested in estimating the number of individuals belonging to each genus found in each plot type. But what if we were interested in the number of genera found in each plot type? Using tally() gives the number of individuals, instead we need to use n_distinct() to count the number of unique values found in a column.\n\n\nspecies <- tbl(mammals, \"species\")\nunique_genera <- left_join(surveys, plots) %>%\n    left_join(species) %>%\n    group_by(plot_type) %>%\n    summarize(\n        n_genera = n_distinct(genus)\n    ) %>%\n    collect()\n\n\n\nn_distinct, like the other dplyr functions we have used in this lesson, works not only on database connections but also on regular data frames.\nCreating a new SQLite database\nSo far, we have used a previously prepared SQLite database. But we can also use R to create a new database, e.g. from existing csv files. Let’s recreate the mammals database that we’ve been working with, in R. First let’s download and read in the csv files. We’ll import tidyverse to gain access to the read_csv() function.\n\n\ndownload.file(\"https://ndownloader.figshare.com/files/3299483\",\n              \"data_raw/species.csv\")\ndownload.file(\"https://ndownloader.figshare.com/files/10717177\",\n              \"data_raw/surveys.csv\")\ndownload.file(\"https://ndownloader.figshare.com/files/3299474\",\n              \"data_raw/plots.csv\")\n\nlibrary(tidyverse)\nspecies <- read_csv(\"data_raw/species.csv\")\nsurveys <- read_csv(\"data_raw/surveys.csv\")\nplots <- read_csv(\"data_raw/plots.csv\")\n\n\n\nCreating a new SQLite database with dplyr is easy. You can re-use the same command we used above to open an existing .sqlite file. The create = TRUE argument instructs R to create a new, empty database instead.\nCaution: When create = TRUE is added, any existing database at the same location is overwritten without warning.\n\n\nmy_db_file <- \"data/portal-database-output.sqlite\"\nmy_db <- src_sqlite(my_db_file, create = TRUE)\n\n\n\nCurrently, our new database is empty, it doesn’t contain any tables:\n\n\nmy_db\n\n\n#> src:  sqlite 3.33.0 [data/portal-database-output.sqlite]\n#> tbls:\n\nTo add tables, we copy the existing data.frames into the database one by one:\n\n\ncopy_to(my_db, surveys)\ncopy_to(my_db, plots)\nmy_db\n\n\n\nIf you check the location of our database you’ll see that data is automatically being written to disk. R and dplyr not only provide easy ways to query existing databases, they also allows you to easily create your own databases from flat files!\n\nChallenge\nAdd the remaining species table to the my_db database and run some of your queries from earlier in the lesson to verify that you have faithfully recreated the mammals database.\n\n\n\n\nNote: In this example, we first loaded all of the data into the R session by reading the three csv files. Because all the data has to flow through R, this is not suitable for very large datasets.\nNote: Finally, to close the connection to the mammals database you may use DBI::dbDisconnect(mammals); this discards all pending work and frees resources, e.g. memory.\n\n\n\n",
      "last_modified": "2020-10-29T19:14:37+00:00"
    },
    {
      "path": "about.html",
      "title": "About this site",
      "description": "Some additional details about the website",
      "author": [],
      "contents": "\nContributors\nThe list of contributors to this lesson is available here.\nMisc.\nQuestions? Feedback? Please file an issue on GitHub\nPage built on 2020-10-29 19:14:37\n\n\n\n",
      "last_modified": "2020-10-29T19:14:37+00:00"
    },
    {
      "path": "CONDUCT.html",
      "title": "Code of Conduct",
      "author": [],
      "contents": "\nAs contributors and maintainers of this project, we pledge to respect all people who contribute through reporting issues, posting feature requests, updating documentation, submitting pull requests or patches, and other activities.\nWe are committed to making participation in this project a harassment-free experience for everyone, regardless of level of experience, gender, gender identity and expression, sexual orientation, disability, personal appearance, body size, race, ethnicity, age, or religion.\nExamples of unacceptable behavior by participants include the use of sexual language or imagery, derogatory comments or personal attacks, trolling, public or private harassment, insults, or other unprofessional conduct.\nProject maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to our Code of Conduct. Project maintainers who do not follow the Code of Conduct may be removed from the project team.\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported by following our reporting guidelines.\n\n\n\n",
      "last_modified": "2020-10-29T19:14:38+00:00"
    },
    {
      "path": "CONTRIBUTING.html",
      "title": "Contributing Guidelines",
      "description": "Some additional details about the website",
      "author": [],
      "contents": "\nData Carpentry in an open source project, and we welcome contributions of all kinds: new lessons, fixes to existing material, bug reports, and reviews of proposed changes are all welcome.\nContributor Agreement\nBy contributing, you agree that we may redistribute your work under our license. In exchange, we will address your issues and/or assess your change proposal as promptly as we can, and help you become a member of our community. Everyone involved in Software Carpentry and Data Carpentry agrees to abide by our code of conduct.\nWorking with GitHub\nSubmitting Issues on GitHub\nIf you have an idea about how to improve the lesson, you can submit it as an issue on GitHub. If you have multiple unrelated suggestions, it is best to open a separate issue for each of them. This makes it easier for the project maintainers to discuss and resolve them.\nSubmitting an issue can count as a contribution for your instructor training checkout. If your contribution is for instructor training, send an email with a link to the issue to checkout@carpentries.org. Please note that it is not necessary to point out in the issue’s title or text that it is a contribution for the instructor training checkout.\nSubmitting Pull Requests\nYou can also suggest changes by modifying the lesson code directly and submitting your changes as a pull request.\nFork the datacarpentry/R-ecology-lesson repository on GitHub. See the “Fork” button in the top-right corner of the screen on the GitHub website.\nClone that repository to your own machine. (It is also possible to make minor edits right on GitHub.) At your terminal:\ngit clone https://github.com/your_username/R-ecology-lesson.git R-ecology-lesson\ncd R-ecology-lesson\ngit remote add upstream https://github.com/datacarpentry/R-ecology-lesson.git\nCreate a branch from main for your changes. Give your branch a meaningful name, such as fix-typos-dplyr-lesson or add-tutorial-on-visualization. At your terminal:\ngit checkout -b fix-typos-dplyr-lesson\nMake your changes to the Rmd file. If you’d like to check the rendered version of your changes, you can do one of three things:\nif you have GNU Make installed on your system, type make at your shell terminal.\nif you use RStudio, click on the “Knit” button in the top-right corner of your editor pane.\nin other cases, you can type: rmarkdown::render_site(\"01-intro-to-r.Rmd\") in your R terminal (make sure your working directory is at the root of the lesson) to generate the corresponding html file.\n\nCommit the Rmd file you edited (git add file-you-changed.Rmd, followed by git commit -m \"fix typos in dplyr lesson\"), and push your changes to your repository on GitHub (git push origin fix-typos-dplyr-lesson). If your change affects a lesson, please only commit and push the Rmd files. The rendered versions will be generated by the lesson maintainers to avoid merge conflicts.\nSend a pull request (PR) to the main branch of the datacarpentry/R-ecology-lesson repository for this lesson at https://github.com/datacarpentry/R-ecology-lesson\nIf you are new to Git or GitHub, software like GitHub Desktop can make this process easier for you.\nIf it is easier for you to send edits to us some other way, please mail us at checkout@carpentries.org. Given a choice between you creating content or wrestling with Git, we’d rather have you doing the former.\nFile Locations and Formats\nRMarkdown\nFor the R material, lessons are written in RMarkdown (files ending in Rmd). Filenames follow the pattern 00-before-we-start.Rmd, 01-intro-to-r.Rmd and so on. That is, we use two digits followed by a topic key to ensure files appear in the right order when listed.\nA Makefile converts the Rmd files into HTML files that are processed by Jekyll (the tool GitHub uses to create websites) as explained in the README file.\nTo ensure a consistent formatting of the lessons, we recommend the following formatting guidelines for RMarkdown files:\nNo trailing white space\nWrap lines at 80 characters (unless it breaks URLs)\nUse consistent capitalization (e.g., R not r, RStudio not rstudio or Rstudio)\nFunction names are written as function() while variables are written as variable, and package names as package.\nUse unclosed atx style headers (see below):\n## Use this format for headers\n\nAnd not this format\n-------------------\nFormatting RMarkdown Code Chunks\nMost R code within .Rmd files is written inside of code chunks. Code chunks can have a name and a number of options, but neither is required. Options are added to a code chunk like this:\n```{r, chunk_name, option1 = value, option2 = value, ...}\nThroughout the lesson, we use different code chunk options, mostly to change when and how the code in the chunks is being executed. Below you will find a list of the most common options we use and information on how we use them. More information on RMarkdown code chunk options can be found here. When in doubt, consult the Rmd files for examples.\nanswer = [FALSE | TRUE]\nThe answer option is used in challenges to hide the content of the chunk so that the reader needs to interact with the website to reveal it. The default value is FALSE.\necho = [FALSE | TRUE]\nIf echo = FALSE, the code will be executed and its output will be visible on the lesson website (unless specified otherwise by the eval, message, or results options), but the code itself will not be visible. This is useful when writing code for the code handout, because it allows to include redundant headings and comments that are not needed in the lesson itself, but help to structure and clarify the code handout. The default value is TRUE.\neval = [FALSE | TRUE]\nIf eval = FALSE the code in the chunk will not be executed by R when the file is processed to create the lesson website. Accordingly, no output will be created. This is useful, for example, when seeing the result of the code is not required for the lesson, or when the code chunk contains code that installs or loads packages, downloads files, or opens the R help window. The default value is TRUE.\nmessage = [FALSE | TRUE]\nIf FALSE messages produced by the code will not be shown. THis is useful, for example when loading packages like tidyverse that output when loaded. By using message = FALSE, such output can be hidden. The default value is TRUE.\npurl = [FALSE | TRUE]\nCode chunks that have the option purl = TRUE will be included in the code handout (see below). The default value is FALSE.\nresults = ['markup' | 'hide' | 'asis' | 'hold']\nDetermines if and how the text output of a code chunk is formatted. Useful values are markup (to format text output using markup, usually formatting it as a code block), asis (to write raw output directly into the document without any markup), and hide (to hide the output, for example when loading data sets).\nCode Handout\nThe code handout code-handout.R contains code that can be distributed to learners. This is particularly useful for error prone code such as long URLs for downloading files. The code handout is created automatically from the lesson’s .Rmd files by make_code_handout.R, and we use the purl() function from knitr to\ncreate the handout. Code that should be included in the code handout must be enclosed in an R code chunk with the chunk option purl = TRUE (see above). To make the handout more useful, consider including explanatory comments.\nData\nWe don’t store data for lessons inside the lesson repositories. For completed lessons the data should be publicly available in a data repository appropriate to the data type. For lesson development the data may be provided in any way that is convenient including posting to a website, on figshare, a public Dropbox link, a GitHub gist, or even included in the pull request (PR). Once the PR is ready to merge the data should be placed in the official data repository and all links to the data updated.\nRaw data go into data_raw/. However, at this stage, this folder is created programmatically and only contain dataset downloaded directly from the Figshare repository. In other words, it can be safely be deleted (e.g. using make clean-data or make clean.)\nThe data/ folder only contains data generated/exported by R code.\nImages\nImages (e.g., screenshots) are stored in the img/ folder. Graphics generated by some R code also go into this folder and get the prefix R-ecology-. This latter case is handled automatically with some knitr options in the setup.R file.\nWebsite Assets\nThe site_libs folder is generated by the rmarkdown package and holds the javascript, css, and fonts used by the website.\nWe aim to have our lessons be as self-contained as possible. Images and other external resources should be included in the repository whenever possible.\nFAQ\nWhere can I get help?  Mail us at team@carpentries.org or come chat with us on Slack.\n\n\n\n",
      "last_modified": "2020-10-29T19:14:39+00:00"
    },
    {
      "path": "index.html",
      "title": "Data Analysis and Visualization in R for Ecologists",
      "author": [],
      "contents": "\n\nContents\nEpisodes\nPreparationsInstall R and RStudio\nUpdate R and RStudio\nInstall required R packages\nDownload the data\n\n\n\n\n\nData Carpentry’s aim is to teach researchers basic concepts, skills, and tools for working with data so that they can get more done in less time, and with less pain. The lessons below were designed for those interested in working with ecology data in R.\nThis is an introduction to R designed for participants with no programming experience. These lessons can be taught in a day (~ 6 hours). They start with some basic information about R syntax, the RStudio interface, and move through how to import CSV files, the structure of data frames, how to deal with factors, how to add/remove rows and columns, how to calculate summary statistics from a data frame, and a brief introduction to plotting. The last lesson demonstrates how to work with databases directly from R.\nThis lesson assumes no prior knowledge of R or RStudio and no programming experience.\nEpisodes\nBefore we start\nIntroduction to R\nStarting with data\nManipulating, analyzing and exporting data with tidyverse\nData visualization with ggplot2\nSQL databases and R\nPreparations\nData Carpentry’s teaching is hands-on, and to follow this lesson learners must have R and RStudio installed on their computers. They also need to be able to install a number of R packages, create directories, and download files. To avoid troubleshooting during the lesson, learners should follow the instruction below to download and install everything beforehand. If they are using their own computers this should be no problem, but if the computer is managed by their organization’s IT department they might need help from an IT administrator.\nInstall R and RStudio\nR and RStudio are two separate pieces of software:\nR is a programming language that is especially powerful for data exploration, visualization, and statistical analysis\nRStudio is an integrated development environment (IDE) that makes using R easier. In this course we use RStudio to interact with R.\nIf you don’t already have R and RStudio installed, follow the instructions for your operating system below. You have to install R before you install RStudio. Once it’s installed, open RStudio to make sure it works and you don’t get any error messages.\nWindows\nDownload R from the CRAN website.\nRun the .exe file that was just downloaded\nGo to the RStudio download page\nUnder Installers select RStudio x.yy.zzz - Windows Vista/7/8/10 (where x, y, and z represent version numbers)\nDouble click the file to install it\nMacOS\nDownload R from the CRAN website.\nSelect the .pkg file for the latest R version\nDouble click on the downloaded file to install R\nIt is also a good idea to install XQuartz (needed by some packages)\nGo to the RStudio download page\nUnder Installers select RStudio x.yy.zzz - Mac OS X 10.6+ (64-bit) (where x, y, and z represent version numbers)\nDouble click the file to install RStudio\nLinux\nFollow the instructions for your distribution from CRAN, they provide information to get the most recent version of R for common distributions. For most distributions, you could use your package manager (e.g., for Debian/Ubuntu run sudo apt-get install r-base, and for Fedora sudo yum install R), but we don’t recommend this approach as the versions provided by this are usually out of date. In any case, make sure you have at least R 3.3.1.\nGo to the RStudio download page\nUnder Installers select the version that matches your distribution, and install it with your preferred method (e.g., with Debian/Ubuntu sudo dpkg -i rstudio-x.yy.zzz-amd64.deb at the terminal).\nUpdate R and RStudio\nIf you already have R and RStudio installed, check if your R and RStudio are up to date:\nWhen you open RStudio your R version will be printed in the console on the bottom left. Alternatively, you can type sessionInfo() into the console. If your R version is 4.0.0 or later, you don’t need to update R for this lesson. If your version of R is older than that, download and install the latest version of R from the R project website for Windows, for MacOS, or for Linux\nTo update RStudio to the latest version, open RStudio and click on Help -> Check for updates. If a new version is available, quit RStudio, follow the instruction on screen.\n\nNote: It is not necessary to remove old versions of R from your system before you update, but if you wish to do so you can find more information here.\nInstall required R packages\nDuring the course we will need a number of R packages. Packages contain useful R code written by other people. We will use the packages tidyverse, hexbin, patchwork, and RSQLite.\nTo try to install these packages, open RStudio and copy and paste the following command into the console window (look for a blinking cursor on the bottom left), then press the Enter (Windows and Linux) or Return (MacOS) to execute the command.\n\n\ninstall.packages(c(\"tidyverse\", \"hexbin\", \"patchwork\", \"RSQLite\"))\n\n\n\nAlternatively, you can install the packages using RStudio’s graphical user interface by going to Tools -> Install Packages and typing the names of the packages separated by a comma.\nR tries to download and install the packages on your machine. When the installation has finished, you can try to load the packages by pasting the following code into the console:\n\n\nlibrary(tidyverse)\nlibrary(hexbin)\nlibrary(patchwork)\nlibrary(RSQLite)\n\n\n\nIf you do not see an error like there is no package called ‘...’ you are good to go!\nDownload the data\nWe will download the data directly from R during the lessons. However, if you are expecting problems with the network, it may be better to download the data beforehand and store it on your machine. The data files for the lesson can be downloaded manually here: https://doi.org/10.6084/m9.figshare.1314459\n\n\n\n",
      "last_modified": "2020-10-29T19:14:39+00:00"
    },
    {
      "path": "instructor-notes.html",
      "title": "Instructor Notes",
      "author": [],
      "contents": "\n\nContents\nDataset\nThe handout\nR Version\nRStudio and Multiple R Installs\nIssues with Fonts on MacOS\nRequired packages\nNarrativeBefore we start\nIntro to R\nStarting with data\nManipulating data\nVisualizing data\nR and SQL\n\nPotential issues & solutions\nTechnical Tips and Tricks\nOther Resources\n\nDataset\nThe data used for this lesson are in the figshare repository at: https://doi.org/10.6084/m9.figshare.1314459\nThis lesson uses mostly combined.csv. The 3 other csv files: plots.csv, species.csv and surveys.csv are only needed for the lesson on databases.\ncombined.csv is downloaded directly in the episode “Starting with Data” and does not need to be downloaded before hand. It however requires that there is a decent internet connection in the room where the workshop is being taught. To facilitate the download process, the chunk of code that includes the URL where the csv file lives, and where the file should go and be named is included in the code handout (see next paragraph). Using this approach ensures that the file will be where the lesson expects it to be, and teaches good/reproducible practice of automating the download. If the learners haven’t created the data/ directory and/or are not in the correct working directory, the download.file command will produce an error. Therefore, it is important to use the stickies at this point.\nThe handout\nThe code handout (a link to download it is also available on the top bar of the lesson website) is useful for Data Carpentry workshops. It includes an outline of the lesson content, the text for the challenges, the links for the files that need to be downloaded for the lesson, and pieces of code that may be difficult to type for learners with no programming experience/who are unfamiliar with R’s syntax. We encourage you to distribute it to the learners at the beginning of the lesson. As an instructor, we encourage you to do the live coding directly in this file, so the participants can follow along.\nR Version\nWith the release of R 4.0.0 in early 2020, an important change has been made to R: The default for stringsAsFactors is now FALSE instead of TRUE. As a result, the read.csv() and data.frame() functions do not automatically convert character columns to factors anymore (you can read more about this here).\nThis change should not cause any problems with this lesson, independent of whether R >4.0 is used or not, because we it uses read_csv() from the tidyverse package throughout. Other than read.csv() from base R, read_csv() never converts character columns to factors, regardless of the R version.\nNevertheless, it is recommended that learners install a version of R ≥4.0.0, and instructors and helpers should be aware of this potential source of error.\nRStudio and Multiple R Installs\nSome learners may have previous R installations. On Mac, if a new install is performed, the learner’s system will create a symbolic link, pointing to the new install as ‘Current.’ Sometimes this process does not occur, and, even though a new R is installed and can be accessed via the R console, RStudio does not find it. The net result of this is that the learner’s RStudio will be running an older R install. This will cause package installations to fail. This can be fixed at the terminal. First, check for the appropriate R installation in the library;\n\n\n\nWe are currently using R 4.0.x. If it isn’t there, they will need to install it. If it is present, you will need to set the symbolic link to Current to point to the 4.0.x directory:\n\n\n\nThen restart RStudio.\nIssues with Fonts on MacOS\nOn older versions of MacOS, it may happen that axis labels do not show up when calling plot() (section “renaming factors” in “Starting with Data”). This issue might be due to the default font Arial being deactivated, so that R cannot find it. To resolve this issue, go to Finder, Search for Font Book and open it. Look for the Arial font and, if it is greyed out, turn it on.\nIf the problem occurs with ggplot2 plots, an alternative workaround is to change the default theme for the R session, so that ggplot uses a serif font. Since Arial is a sans-serif font, R will try to load a different font. This can be done with theme_update(text = element_text(family = \"serif\")).\nRequired packages\nSave yourself some aggrevation, and have everyone check and see if they can install all these packages before you start the first day. See the “Preparations” section on the homepage of the course website for package installation instructions.\nSometimes learners are unable to install the tidyverse package. In that case, they can try to install the individual packages that are actually needed:\n\n\n\nNarrative\nBefore we start\nThe main goal here is to help the learners be comfortable with the RStudio interface. We use RStudio because it helps make using R more organized and user friendly.\nThe “Why learning R?” section contains suggestions of what you could tell your learners about the benefits of learning R. However, it’s best if you can talk here about what has worked for you personally.\nGo very slowly in the “Getting setup section”. Make sure everyone is following along (remind learners to use the stickies). Plan with the helpers at this point to go around the room, and be available to help. It’s important to make sure that learners are in the correct working directory, and that they create a data_raw (all lowercase) subfolder.\nThe seeking help section is relatively long, and while it’s useful to demonstrate a couple of ways to get help from within R, you may want to mostly point the workshop participants to this useful reference so that they can refer to it after the workshop.\nIn the “where to ask for help section?”, you may want to emphasize the first point about how workshops are a great way to create community of learners that can help each others during and after the workshop.\nIntro to R\nWhen going over the section on assignments, make sure to pause for at least 30 seconds when asking “What do you think is the current content of the object weight_lb? 126.5 or 220?”. For learners with no programming experience, this is a new and important concept.\nGiven that the concept of missing data is an important feature of the R language, it is worth spending enough time on it.\nStarting with data\nThe two main goals for this lessons are:\nTo make sure that learners are comfortable with working with data frames, and can use the bracket notation to select slices/columns\nTo expose learners to factors. Their behavior is not necessarily intuitive, and so it is important that they are guided through it the first time they are exposed to it. The content of the lesson should be enough for learners to avoid common mistakes with them.\nManipulating data\nFor this lesson make sure that learners are comfortable using pipes.\nThere is also sometimes some confusion on what the arguments of group_by should be.\nThis lesson uses the tidyr package to reshape data for plotting\nAfter this lesson students should be familiar with the spread() and gather() functions available in tidyr\nWhile working with the example for mutate(), it is difficult to see the “weight” columns on a zoomed in RStudio screen. Including a select() command to select the columns “weight_kg” and “weight_lb” makes it easier to view how the “weight” columns are changed.\nClarify the differences between the functions read_csv() (used in this lesson) and read.csv() (used in the previou lesson).\nVisualizing data\nThis lesson is a broad overview of ggplot2 and focuses on (1) getting familiar with the layering system of ggplot2, (2) using the argument group in the aes() function, (3) basic customization of the plots.\nIt maybe worthwhile to mention that we can also specify colors by color HEX code (http://colorbrewer2.org)\n\n\n\nR and SQL\nIdeally this lesson is best taught at the end of the workshop (as a capstone example) to illustrate how the tools covered can integrate with each others. Depending on the audience, and the pace of the workshop, it can be shown as a demonstration rather than a typically lesson.\nThe explanation of how dplyr’s verb syntax is translated into SQL statements, and the section on laziness are optional and don’t need to be taught in detail during a workshop. They can be useful after a workshop for learners interested in learning more about the topics or for instructors to answer questions from the workshop participants.\nPotential issues & solutions\nAs it stands, the solutions to all the challenges are commented out in the Rmd files. If you want to double check your answer, you can look at the source code of the Rmd files on GitHub.\nTechnical Tips and Tricks\nShow how to use the ‘zoom’ button to blow up graphs without constantly resizing windows\nSometimes a package will not install, try a different CRAN mirror - Tools > Global Options > Packages > CRAN Mirror\nAlternatively you can go to CRAN and download the package and install from ZIP file - Tools > Install Packages > set to ‘from Zip/TAR’\nIt is important that R, and the R packages be installed locally, not on a network drive. If a learner is using a machine with multiple users where their account is not based locally this can create a variety of issues (This often happens on university computers). Hopefully the learner will realize these issues before hand, but depending on the machine and how the IT folks that service the computer have things set up, it may be very difficult to impossible to make R work without their help.\nIf learners are having issues with one package, they may have issues with another. Its often easier to make sure they have all the needed packages installed at one time, rather then deal with these issues over and over. Here is a list of all necessary packages for these lessons.\nOther Resources\nIf you encounter a problem during a workshop, feel free to contact the maintainers by email or open an issue.\nFor a more in-depth coverage of topics of the workshops, you may want to read “R for Data Science” by Hadley Wickham and Garrett Grolemund.\n\n\n\n",
      "last_modified": "2020-10-29T19:14:40+00:00"
    },
    {
      "path": "LICENSE.html",
      "title": "License",
      "author": [],
      "contents": "\nInstructional Material\nAll Data Carpentry instructional material is made available under the Creative Commons Attribution license. You are free:\nto Share—to copy, distribute and transmit the work\nto Remix—to adapt the work\nUnder the following conditions:\nAttribution—You must attribute the work using “Copyright (c) Data Carpentry” (but not in any way that suggests that we endorse you or your use of the work). Where practical, you must also include a hyperlink to https://datacarpentry.org.\nWith the understanding that:\nWaiver—Any of the above conditions can be waived if you get permission from the copyright holder.\nOther Rights—In no way are any of the following rights affected by the license:\nYour fair dealing or fair use rights;\nThe author’s moral rights;\nRights other persons may have either in the work itself or in how the work is used, such as publicity or privacy rights. *\n\nNotice—For any reuse or distribution, you must make clear to others the license terms of this work. The best way to do this is with a link to https://creativecommons.org/licenses/by/4.0/.\nFor the full legal text of this license, please see https://creativecommons.org/licenses/by/4.0/legalcode.\nSoftware\nExcept where otherwise noted, the example programs and other software provided by Data Carpentry are made available under the OSI-approved MIT license.\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n\n",
      "last_modified": "2020-10-29T19:14:41+00:00"
    },
    {
      "path": "NEWS.html",
      "author": [],
      "contents": "\nR-ecology-lesson v2017.04.0\nInitial release of the Data Carpentry R ecology lesson.\n\n\n",
      "last_modified": "2020-10-29T19:14:41+00:00"
    },
    {
      "path": "README.html",
      "title": "README",
      "author": [],
      "contents": "\n  \nData carpentry: R for data analysis and visualization of Ecological Data\nThis is an introduction to R designed for participants with no programming experience. It can be taught in 3/4 of a day (approximately 6 hours). The lesson starts with some basic information about syntax for the R programming language, the RStudio interface, and moves through to specific programming tasks, such as importing CSV files, the structure of data frame objects in R, dealing with categorical variables (i.e. factors), basic data manipulation (adding/removing rows and columns), and finishing with calculating summary statistics and a brief introduction to plotting. There is also a lesson on how to use databases from R that is intended to be taught after the SQL lesson, and ideally at the end of a Data Carpentry workshop.\nPrerequisites\nThe lesson assumes no prior knowledge of R or RStudio. Learners should have R and RStudio installed on their computers. They will also need to be able to install R packages from CRAN, create directories, and download files. See the lesson website for instructions on installing R, RStudio, and the required R packages.\nTopics\nBefore we start\nIntroduction to R\nStarting with data\nManipulating, analyzing and exporting data with tidyverse\nData visualization with ggplot2\nSQL databases and R\nCode handout\nThere is a code handout that is intended to be distributed to the participants. This file includes some of the examples used during teaching and the titles of the section. It provides a guide that the participants can fill in as the lesson progresses. Participants can also source code from this file to avoid typos in more complex examples.\nContributing\nContributions to the content and development of these lesson are very welcome! If you would like to contribute, we encourage you to review our contributing guide.\nQuestions\nIf you have any questions or feedback, please open an issue, contact the maintainers, or come chat with us on the Slack Channel for this lesson. If you don’t already have a Slack account with the Carpentries, you can create one.\nAna Costa Conrado\nFrançois Michonneau\nManeesha Sane\nBrian Seok\nAshwin Srinath\nCitation\nPlease cite as\n\nFrançois Michonneau, Tracy Teal, Auriel Fournier, Brian Seok, Adam Obeng, Aleksandra Natalia Pawlik, … Ye Li. (2019, July 1). datacarpentry/R-ecology-lesson: Data Carpentry: Data Analysis and Visualization in R for Ecologists, June 2019 (Version v2019.06.1). Zenodo. http://doi.org/10.5281/zenodo.3264888\n\n\n\n\n",
      "last_modified": "2020-10-29T19:14:42+00:00"
    },
    {
      "path": "reference.html",
      "author": [],
      "contents": "\nCheat sheet of functions used in the lessons\nLesson 1 – Introduction to R\nsqrt() # calculate the square root\nround() # round a number\nargs() # find what arguments a function takes\nlength() # how many elements are in a particular vector\nclass() # the class (the type of element) of an object\nstr() # an overview of the object and the elements it contains\nc() # create vector; add elements to vector\n[  ] # extract and subset vector\n%in% # to test if a value is found in a vector\nis.na() # test if there are missing values\nna.omit() # Returns the object with incomplete cases removed\ncomplete.cases()# elements which are complete cases\nLesson 2 – Starting with data\ndownload.file() # download files from the internet to your computer\nread_csv() # load CSV file into R memory\nhead() # shows the first 6 rows\nview() # invoke a spreadsheet-style data viewer\nread_delim() # load a file in table format into R memory\nstr() # check structure of the object and information about the class, length and content of each column\ndim() # check dimension of data frame\nnrow() # returns the number of rows\nncol() # returns the number of columns\ntail() # shows the last 6 rows\nnames() # returns the column names (synonym of colnames() for data frame objects)\nrownames() # returns the row names\nsummary() # summary statistics for each column\nfactor() # create factors\nlevels() # check levels of a factor\nnlevels() # check number of levels of a factor\nas.character() # convert an object to a character vector\nas.numeric() # convert an object to a numeric vector\nas.numeric(as.character(x)) # convert factors where the levels appear as characters to a numeric vector\nas.numeric(levels(x))[x] # convert factors where the levels appear as numbers to a numeric vector\nplot() # plot an object\naddNA() # convert NA into a factor level\ndata.frame() # create a data.frame object\nymd() # convert a vector representing year, month, and day to a Date vector\npaste() # concatenate vectors after converting to character\nLesson 3 – Manipulating, analyzing and exporting data with tidyverse\nstr() # check structure of the object and information about the class, length and content of each column\nview() # invoke a spreadsheet-style data viewer\nselect() # select columns of a data frame\nfilter() # allows you to select a subset of rows in a data frame\n%>% # pipes to select and filter at the same time\nmutate() # create new columns based on the values in existing columns\nhead() # shows the first 6 rows\ngroup_by() # split the data into groups, apply some analysis to each group, and then combine the results.\nsummarize() # collapses each group into a single-row summary of that group\nmean() # calculate the mean value of a vector\n!is.na() # test if there are no missing values\nprint() # print values to the console\nmin() # return the minimum value of a vector\narrange() # arrange rows by variables\ndesc() # transform a vector into a format that will be sorted in descending order\ncount() # counts the total number of records for each category\nspread() # reshape a data frame by a key-value pair across multiple columns\ngather() # reshape a data frame by collapsing into a key-value pair\nn_distinct() # get a count of unique values\nwrite_csv() # save to a csv formatted file\nLesson 4 – Data visualization with ggplot2\nread_csv() # load a csv formatted file into R memory\nggplot2(data= , aes(x= , y= )) + geom_point( ) + facet_wrap () + theme_bw() + theme()\naes() # by selecting the variables to be plotted and the variables to define the presentation such as plotting size, shape color, etc.\ngeom_ # graphical representation of the data in the plot (points, lines, bars). To add a geom to the plot use + operator\nfacet_wrap() # allows to split one plot into multiple plots based on a factor included in the dataset\nlabs() # set labels to plot\ntheme_bw() # set the background to white\ntheme() # used to locally modify one or more theme elements in a specific ggplot object\n+ # arrange ggplots horizontally\n/ # arrange ggplots vertically\nplot_layout() # set width and height of individual plots in a patchwork of plots\nggsave() # save a ggplot\nLesson 5 – SQL databases and R\ndir.create() # create a directory\ndownload.file() # download files from the internet to your computer\ndbConnect() # create a connection to a database\nSQLite() # connect to a SQLite database\nsrc_dbi() # connect dplyr to a DBI-compatible database file\ntbl # connect to a table within a database\nsql() # combine character vectors into a single SQL expression\nshow_query() # show which SQL commands are sent to the database\ncollect() # retrieve all the results from the database\ninner_join() # perform an inner join between two tables\nsrc_sqlite() # connect dplyr to a SQLite database file\ncopy_to() # copy a data frame as a table into a database\n",
      "last_modified": "2020-10-29T19:14:42+00:00"
    }
  ],
  "collections": []
}
